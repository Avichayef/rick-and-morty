
==> Audit <==
|--------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|
|   Command    |              Args              | Profile  |   User    | Version |     Start Time      |      End Time       |
|--------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|
| start        | --driver=docker                | minikube | avichayef | v1.35.0 | 13 Mar 25 14:14 IST | 13 Mar 25 14:22 IST |
| ip           |                                | minikube | avichayef | v1.35.0 | 14 Mar 25 10:22 IST |                     |
| ip           |                                | minikube | avichayef | v1.35.0 | 14 Mar 25 10:54 IST |                     |
| start        |                                | minikube | avichayef | v1.35.0 | 17 Mar 25 19:23 IST | 17 Mar 25 19:28 IST |
| tunnel       |                                | minikube | avichayef | v1.35.0 | 17 Mar 25 20:20 IST | 17 Mar 25 20:24 IST |
| delete       |                                | minikube | avichayef | v1.35.0 | 17 Mar 25 20:24 IST | 17 Mar 25 20:25 IST |
| start        |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 08:16 IST | 18 Mar 25 08:20 IST |
| start        |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 13:09 IST | 18 Mar 25 13:14 IST |
| start        |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:08 IST | 18 Mar 25 15:13 IST |
| addons       | enable ingress                 | minikube | avichayef | v1.35.0 | 18 Mar 25 15:13 IST |                     |
| addons       | enable ingress                 | minikube | avichayef | v1.35.0 | 18 Mar 25 15:19 IST |                     |
| addons       | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 15:23 IST | 18 Mar 25 15:23 IST |
| stop         |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:25 IST | 18 Mar 25 15:25 IST |
| start        |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:25 IST | 18 Mar 25 15:29 IST |
| addons       | enable ingress                 | minikube | avichayef | v1.35.0 | 18 Mar 25 15:29 IST | 18 Mar 25 15:35 IST |
| addons       | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 15:35 IST | 18 Mar 25 15:35 IST |
| ip           |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:38 IST | 18 Mar 25 15:38 IST |
| ip           |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:38 IST | 18 Mar 25 15:38 IST |
| docker-env   |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:39 IST | 18 Mar 25 15:39 IST |
| docker-env   |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:42 IST | 18 Mar 25 15:42 IST |
| image        | load rick-morty-service:latest | minikube | avichayef | v1.35.0 | 18 Mar 25 15:47 IST |                     |
| stop         |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:48 IST | 18 Mar 25 15:49 IST |
| delete       |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 15:49 IST | 18 Mar 25 15:49 IST |
| stop         |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:05 IST |                     |
| delete       |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:05 IST | 18 Mar 25 17:05 IST |
| start        |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:07 IST | 18 Mar 25 17:10 IST |
| docker-env   |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:09 IST | 18 Mar 25 17:10 IST |
| addons       | enable ingress                 | minikube | avichayef | v1.35.0 | 18 Mar 25 17:11 IST |                     |
| stop         |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:15 IST | 18 Mar 25 17:16 IST |
| delete       |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:16 IST | 18 Mar 25 17:16 IST |
| start        | --cpus=4                       | minikube | avichayef | v1.35.0 | 18 Mar 25 17:16 IST | 18 Mar 25 17:19 IST |
| addons       | enable ingress                 | minikube | avichayef | v1.35.0 | 18 Mar 25 17:20 IST |                     |
| addons       | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 17:22 IST | 18 Mar 25 17:22 IST |
| addons       | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 17:23 IST | 18 Mar 25 17:23 IST |
| addons       | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 17:24 IST | 18 Mar 25 17:24 IST |
| addons       | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 17:25 IST | 18 Mar 25 17:25 IST |
| delete       |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:29 IST | 18 Mar 25 17:30 IST |
| start        | --memory=2048 --cpus=2         | minikube | avichayef | v1.35.0 | 18 Mar 25 17:30 IST | 18 Mar 25 17:32 IST |
| start        | --driver=docker                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:33 IST |                     |
| stop         |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:34 IST | 18 Mar 25 17:34 IST |
| delete       |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:34 IST | 18 Mar 25 17:34 IST |
| start        | --driver=docker --cpus=4       | minikube | avichayef | v1.35.0 | 18 Mar 25 17:35 IST | 18 Mar 25 17:38 IST |
|              | --memory=8192 --disk-size=50g  |          |           |         |                     |                     |
| addons       | enable ingress                 | minikube | avichayef | v1.35.0 | 18 Mar 25 17:39 IST |                     |
| addons       | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 17:47 IST | 18 Mar 25 17:47 IST |
| addons       | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 17:52 IST | 18 Mar 25 17:52 IST |
| addons       | enable ingress                 | minikube | avichayef | v1.35.0 | 18 Mar 25 17:54 IST |                     |
| update-check |                                | minikube | avichayef | v1.35.0 | 18 Mar 25 17:58 IST | 18 Mar 25 17:58 IST |
| service      | list                           | minikube | avichayef | v1.35.0 | 18 Mar 25 17:58 IST | 18 Mar 25 17:58 IST |
|--------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/18 17:35:38
Running on machine: newserver4
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0318 17:35:38.002120   55608 out.go:345] Setting OutFile to fd 1 ...
I0318 17:35:38.003101   55608 out.go:397] isatty.IsTerminal(1) = true
I0318 17:35:38.003118   55608 out.go:358] Setting ErrFile to fd 2...
I0318 17:35:38.003122   55608 out.go:397] isatty.IsTerminal(2) = true
I0318 17:35:38.003276   55608 root.go:338] Updating PATH: /home/avichayef/.minikube/bin
I0318 17:35:38.003298   55608 oci.go:582] shell is pointing to dockerd inside minikube. will unset to use host
I0318 17:35:38.003710   55608 out.go:352] Setting JSON to false
I0318 17:35:38.005513   55608 start.go:129] hostinfo: {"hostname":"newserver4","uptime":13769,"bootTime":1742298369,"procs":225,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-52-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"2f251221-25b1-4e5e-bf9a-b75c03166920"}
I0318 17:35:38.005596   55608 start.go:139] virtualization: vbox guest
I0318 17:35:38.014785   55608 out.go:177] üòÑ  minikube v1.35.0 on Ubuntu 22.04 (vbox/amd64)
I0318 17:35:38.023173   55608 notify.go:220] Checking for updates...
I0318 17:35:38.069168   55608 out.go:177]     ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
I0318 17:35:38.077985   55608 driver.go:394] Setting default libvirt URI to qemu:///system
I0318 17:35:38.381285   55608 docker.go:123] docker version: linux-28.0.1:Docker Engine - Community
I0318 17:35:38.381628   55608 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0318 17:35:38.912263   55608 info.go:266] docker info: {ID:649807dd-edb8-4b65-8ad5-543487eb6702 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:22 OomKillDisable:false NGoroutines:42 SystemTime:2025-03-18 17:35:38.811900608 +0200 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-52-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:18337603584 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:newserver4 Labels:[] ExperimentalBuild:false ServerVersion:28.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.21.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.33.1]] Warnings:<nil>}}
I0318 17:35:38.912339   55608 docker.go:318] overlay module found
I0318 17:35:38.939170   55608 out.go:177] ‚ú®  Using the docker driver based on user configuration
I0318 17:35:38.945012   55608 start.go:297] selected driver: docker
I0318 17:35:38.945019   55608 start.go:901] validating driver "docker" against <nil>
I0318 17:35:38.945027   55608 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0318 17:35:38.946687   55608 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0318 17:35:39.636203   55608 info.go:266] docker info: {ID:649807dd-edb8-4b65-8ad5-543487eb6702 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:22 OomKillDisable:false NGoroutines:42 SystemTime:2025-03-18 17:35:39.574769142 +0200 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-52-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:18337603584 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:newserver4 Labels:[] ExperimentalBuild:false ServerVersion:28.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.21.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.33.1]] Warnings:<nil>}}
I0318 17:35:39.639171   55608 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0318 17:35:39.639355   55608 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0318 17:35:39.654926   55608 out.go:177] üìå  Using Docker driver with root privileges
I0318 17:35:39.664184   55608 cni.go:84] Creating CNI manager for ""
I0318 17:35:39.664257   55608 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0318 17:35:39.664268   55608 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0318 17:35:39.664339   55608 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8192 CPUs:4 DiskSize:51200 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/avichayef:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0318 17:35:39.683784   55608 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0318 17:35:39.691158   55608 cache.go:121] Beginning downloading kic base image for docker with docker
I0318 17:35:39.700726   55608 out.go:177] üöú  Pulling base image v0.0.46 ...
I0318 17:35:39.710284   55608 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0318 17:35:39.710325   55608 preload.go:146] Found local preload: /home/avichayef/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0318 17:35:39.710333   55608 cache.go:56] Caching tarball of preloaded images
I0318 17:35:39.710804   55608 preload.go:172] Found /home/avichayef/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0318 17:35:39.710814   55608 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0318 17:35:39.710902   55608 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0318 17:35:39.711080   55608 profile.go:143] Saving config to /home/avichayef/.minikube/profiles/minikube/config.json ...
I0318 17:35:39.711094   55608 lock.go:35] WriteFile acquiring /home/avichayef/.minikube/profiles/minikube/config.json: {Name:mk6894cb968884047c3bfd3324d815ddc8408080 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:35:39.834737   55608 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0318 17:35:39.834747   55608 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0318 17:35:39.834756   55608 cache.go:227] Successfully downloaded all kic artifacts
I0318 17:35:39.834774   55608 start.go:360] acquireMachinesLock for minikube: {Name:mk93a0cd601680089cbac239f4cf225177bc90ea Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0318 17:35:39.834813   55608 start.go:364] duration metric: took 29.337¬µs to acquireMachinesLock for "minikube"
I0318 17:35:39.834825   55608 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8192 CPUs:4 DiskSize:51200 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/avichayef:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0318 17:35:39.834878   55608 start.go:125] createHost starting for "" (driver="docker")
I0318 17:35:39.843510   55608 out.go:235] üî•  Creating docker container (CPUs=4, Memory=8192MB) ...
I0318 17:35:39.843894   55608 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0318 17:35:39.843915   55608 client.go:168] LocalClient.Create starting
I0318 17:35:39.844544   55608 main.go:141] libmachine: Reading certificate data from /home/avichayef/.minikube/certs/ca.pem
I0318 17:35:39.844574   55608 main.go:141] libmachine: Decoding PEM data...
I0318 17:35:39.844583   55608 main.go:141] libmachine: Parsing certificate...
I0318 17:35:39.844632   55608 main.go:141] libmachine: Reading certificate data from /home/avichayef/.minikube/certs/cert.pem
I0318 17:35:39.844731   55608 main.go:141] libmachine: Decoding PEM data...
I0318 17:35:39.844742   55608 main.go:141] libmachine: Parsing certificate...
I0318 17:35:39.845262   55608 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0318 17:35:40.147900   55608 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0318 17:35:40.147968   55608 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0318 17:35:40.147979   55608 cli_runner.go:164] Run: docker network inspect minikube
W0318 17:35:40.283974   55608 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0318 17:35:40.283989   55608 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0318 17:35:40.284008   55608 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0318 17:35:40.284115   55608 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0318 17:35:40.413549   55608 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0019de550}
I0318 17:35:40.413578   55608 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0318 17:35:40.413618   55608 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0318 17:35:41.297439   55608 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0318 17:35:41.297455   55608 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0318 17:35:41.297544   55608 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0318 17:35:41.660822   55608 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0318 17:35:42.059024   55608 oci.go:103] Successfully created a docker volume minikube
I0318 17:35:42.059155   55608 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0318 17:35:44.879226   55608 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib: (2.819998978s)
I0318 17:35:44.879249   55608 oci.go:107] Successfully prepared a docker volume minikube
I0318 17:35:44.879279   55608 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0318 17:35:44.879299   55608 kic.go:194] Starting extracting preloaded images to volume ...
I0318 17:35:44.879365   55608 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/avichayef/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0318 17:36:01.873900   55608 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/avichayef/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (16.994481331s)
I0318 17:36:01.881979   55608 kic.go:203] duration metric: took 17.002670009s to extract preloaded images to volume ...
W0318 17:36:01.882086   55608 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0318 17:36:01.882115   55608 oci.go:249] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0318 17:36:01.882151   55608 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0318 17:36:02.873443   55608 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=8192mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0318 17:36:06.340427   55608 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=8192mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279: (3.466916809s)
I0318 17:36:06.340517   55608 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0318 17:36:06.605662   55608 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0318 17:36:07.199499   55608 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0318 17:36:08.341065   55608 cli_runner.go:217] Completed: docker exec minikube stat /var/lib/dpkg/alternatives/iptables: (1.141536736s)
I0318 17:36:08.341087   55608 oci.go:144] the created container "minikube" has a running status.
I0318 17:36:08.341098   55608 kic.go:225] Creating ssh key for kic: /home/avichayef/.minikube/machines/minikube/id_rsa...
I0318 17:36:11.380513   55608 kic_runner.go:191] docker (temp): /home/avichayef/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0318 17:36:13.008045   55608 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0318 17:36:13.612138   55608 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0318 17:36:13.612152   55608 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0318 17:36:15.402624   55608 kic_runner.go:123] Done: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]: (1.790450499s)
I0318 17:36:15.402701   55608 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0318 17:36:15.765440   55608 machine.go:93] provisionDockerMachine start ...
I0318 17:36:15.765530   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:16.292541   55608 main.go:141] libmachine: Using SSH client type: native
I0318 17:36:16.292794   55608 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32793 <nil> <nil>}
I0318 17:36:16.292801   55608 main.go:141] libmachine: About to run SSH command:
hostname
I0318 17:36:17.007306   55608 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0318 17:36:17.007567   55608 ubuntu.go:169] provisioning hostname "minikube"
I0318 17:36:17.007649   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:17.825505   55608 main.go:141] libmachine: Using SSH client type: native
I0318 17:36:17.825671   55608 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32793 <nil> <nil>}
I0318 17:36:17.825678   55608 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0318 17:36:18.645477   55608 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0318 17:36:18.645555   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:19.039411   55608 main.go:141] libmachine: Using SSH client type: native
I0318 17:36:19.039651   55608 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32793 <nil> <nil>}
I0318 17:36:19.039680   55608 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0318 17:36:20.040605   55608 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0318 17:36:20.040630   55608 ubuntu.go:175] set auth options {CertDir:/home/avichayef/.minikube CaCertPath:/home/avichayef/.minikube/certs/ca.pem CaPrivateKeyPath:/home/avichayef/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/avichayef/.minikube/machines/server.pem ServerKeyPath:/home/avichayef/.minikube/machines/server-key.pem ClientKeyPath:/home/avichayef/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/avichayef/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/avichayef/.minikube}
I0318 17:36:20.040946   55608 ubuntu.go:177] setting up certificates
I0318 17:36:20.040959   55608 provision.go:84] configureAuth start
I0318 17:36:20.041052   55608 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0318 17:36:20.537162   55608 provision.go:143] copyHostCerts
I0318 17:36:20.537242   55608 exec_runner.go:144] found /home/avichayef/.minikube/ca.pem, removing ...
I0318 17:36:20.537251   55608 exec_runner.go:203] rm: /home/avichayef/.minikube/ca.pem
I0318 17:36:20.537347   55608 exec_runner.go:151] cp: /home/avichayef/.minikube/certs/ca.pem --> /home/avichayef/.minikube/ca.pem (1086 bytes)
I0318 17:36:20.537482   55608 exec_runner.go:144] found /home/avichayef/.minikube/cert.pem, removing ...
I0318 17:36:20.537487   55608 exec_runner.go:203] rm: /home/avichayef/.minikube/cert.pem
I0318 17:36:20.537532   55608 exec_runner.go:151] cp: /home/avichayef/.minikube/certs/cert.pem --> /home/avichayef/.minikube/cert.pem (1127 bytes)
I0318 17:36:20.537617   55608 exec_runner.go:144] found /home/avichayef/.minikube/key.pem, removing ...
I0318 17:36:20.537622   55608 exec_runner.go:203] rm: /home/avichayef/.minikube/key.pem
I0318 17:36:20.537660   55608 exec_runner.go:151] cp: /home/avichayef/.minikube/certs/key.pem --> /home/avichayef/.minikube/key.pem (1679 bytes)
I0318 17:36:20.537735   55608 provision.go:117] generating server cert: /home/avichayef/.minikube/machines/server.pem ca-key=/home/avichayef/.minikube/certs/ca.pem private-key=/home/avichayef/.minikube/certs/ca-key.pem org=avichayef.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0318 17:36:20.821940   55608 provision.go:177] copyRemoteCerts
I0318 17:36:20.822024   55608 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0318 17:36:20.822076   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:21.385723   55608 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32793 SSHKeyPath:/home/avichayef/.minikube/machines/minikube/id_rsa Username:docker}
I0318 17:36:21.704925   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0318 17:36:22.006862   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/machines/server.pem --> /etc/docker/server.pem (1188 bytes)
I0318 17:36:22.174845   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0318 17:36:22.345967   55608 provision.go:87] duration metric: took 2.304990714s to configureAuth
I0318 17:36:22.345990   55608 ubuntu.go:193] setting minikube options for container-runtime
I0318 17:36:22.346248   55608 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0318 17:36:22.346315   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:22.486900   55608 main.go:141] libmachine: Using SSH client type: native
I0318 17:36:22.487298   55608 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32793 <nil> <nil>}
I0318 17:36:22.487311   55608 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0318 17:36:22.794585   55608 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0318 17:36:22.794596   55608 ubuntu.go:71] root file system type: overlay
I0318 17:36:22.794705   55608 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0318 17:36:22.795781   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:22.961196   55608 main.go:141] libmachine: Using SSH client type: native
I0318 17:36:22.961946   55608 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32793 <nil> <nil>}
I0318 17:36:22.962753   55608 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0318 17:36:23.348316   55608 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0318 17:36:23.348411   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:23.553013   55608 main.go:141] libmachine: Using SSH client type: native
I0318 17:36:23.553894   55608 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32793 <nil> <nil>}
I0318 17:36:23.553920   55608 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0318 17:36:28.636727   55608 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-03-18 15:36:23.339329546 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0318 17:36:28.636753   55608 machine.go:96] duration metric: took 12.871299741s to provisionDockerMachine
I0318 17:36:28.636773   55608 client.go:171] duration metric: took 48.792850834s to LocalClient.Create
I0318 17:36:28.636791   55608 start.go:167] duration metric: took 48.792894209s to libmachine.API.Create "minikube"
I0318 17:36:28.636798   55608 start.go:293] postStartSetup for "minikube" (driver="docker")
I0318 17:36:28.636826   55608 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0318 17:36:28.636916   55608 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0318 17:36:28.636974   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:28.789810   55608 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32793 SSHKeyPath:/home/avichayef/.minikube/machines/minikube/id_rsa Username:docker}
I0318 17:36:29.104219   55608 ssh_runner.go:195] Run: cat /etc/os-release
I0318 17:36:29.142652   55608 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0318 17:36:29.142681   55608 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0318 17:36:29.142700   55608 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0318 17:36:29.142707   55608 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0318 17:36:29.142718   55608 filesync.go:126] Scanning /home/avichayef/.minikube/addons for local assets ...
I0318 17:36:29.142809   55608 filesync.go:126] Scanning /home/avichayef/.minikube/files for local assets ...
I0318 17:36:29.142847   55608 start.go:296] duration metric: took 506.042707ms for postStartSetup
I0318 17:36:29.143620   55608 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0318 17:36:29.252817   55608 profile.go:143] Saving config to /home/avichayef/.minikube/profiles/minikube/config.json ...
I0318 17:36:29.254248   55608 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0318 17:36:29.254299   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:29.386564   55608 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32793 SSHKeyPath:/home/avichayef/.minikube/machines/minikube/id_rsa Username:docker}
I0318 17:36:29.569572   55608 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0318 17:36:29.593872   55608 start.go:128] duration metric: took 49.758979288s to createHost
I0318 17:36:29.593889   55608 start.go:83] releasing machines lock for "minikube", held for 49.759070253s
I0318 17:36:29.594486   55608 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0318 17:36:29.944931   55608 ssh_runner.go:195] Run: cat /version.json
I0318 17:36:29.944982   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:29.945802   55608 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0318 17:36:29.945870   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:36:30.299914   55608 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32793 SSHKeyPath:/home/avichayef/.minikube/machines/minikube/id_rsa Username:docker}
I0318 17:36:30.311425   55608 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32793 SSHKeyPath:/home/avichayef/.minikube/machines/minikube/id_rsa Username:docker}
I0318 17:36:30.481838   55608 ssh_runner.go:195] Run: systemctl --version
I0318 17:36:30.886537   55608 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0318 17:36:30.901444   55608 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0318 17:36:30.991759   55608 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0318 17:36:30.991812   55608 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0318 17:36:31.131932   55608 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0318 17:36:31.131957   55608 start.go:495] detecting cgroup driver to use...
I0318 17:36:31.132003   55608 detect.go:190] detected "systemd" cgroup driver on host os
I0318 17:36:31.132120   55608 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0318 17:36:31.183403   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0318 17:36:31.208062   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0318 17:36:31.266196   55608 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0318 17:36:31.266265   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0318 17:36:31.315741   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0318 17:36:31.374090   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0318 17:36:31.403535   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0318 17:36:31.444461   55608 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0318 17:36:31.517614   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0318 17:36:31.586803   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0318 17:36:31.623802   55608 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0318 17:36:31.664353   55608 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0318 17:36:31.701154   55608 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0318 17:36:31.726866   55608 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0318 17:36:31.874844   55608 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0318 17:36:32.320232   55608 start.go:495] detecting cgroup driver to use...
I0318 17:36:32.320275   55608 detect.go:190] detected "systemd" cgroup driver on host os
I0318 17:36:32.320337   55608 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0318 17:36:32.499911   55608 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0318 17:36:32.499990   55608 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0318 17:36:32.630914   55608 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0318 17:36:32.805748   55608 ssh_runner.go:195] Run: which cri-dockerd
I0318 17:36:32.890300   55608 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0318 17:36:33.113598   55608 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0318 17:36:33.226516   55608 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0318 17:36:33.630133   55608 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0318 17:36:33.864832   55608 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0318 17:36:33.865571   55608 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0318 17:36:33.969736   55608 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0318 17:36:34.347018   55608 ssh_runner.go:195] Run: sudo systemctl restart docker
I0318 17:36:39.953647   55608 ssh_runner.go:235] Completed: sudo systemctl restart docker: (5.606608105s)
I0318 17:36:39.953698   55608 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0318 17:36:40.019907   55608 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0318 17:36:40.065760   55608 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0318 17:36:40.268830   55608 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0318 17:36:40.484825   55608 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0318 17:36:40.657726   55608 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0318 17:36:40.738932   55608 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0318 17:36:40.770907   55608 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0318 17:36:41.009087   55608 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0318 17:36:41.732215   55608 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0318 17:36:41.732280   55608 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0318 17:36:41.900472   55608 start.go:563] Will wait 60s for crictl version
I0318 17:36:41.900527   55608 ssh_runner.go:195] Run: which crictl
I0318 17:36:41.937516   55608 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0318 17:36:42.221399   55608 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0318 17:36:42.221460   55608 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0318 17:36:42.535776   55608 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0318 17:36:42.773020   55608 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0318 17:36:42.773908   55608 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0318 17:36:42.909743   55608 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0318 17:36:42.923697   55608 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0318 17:36:42.954429   55608 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8192 CPUs:4 DiskSize:51200 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/avichayef:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0318 17:36:42.954533   55608 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0318 17:36:42.954600   55608 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0318 17:36:43.110157   55608 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0318 17:36:43.110165   55608 docker.go:619] Images already preloaded, skipping extraction
I0318 17:36:43.110216   55608 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0318 17:36:43.247651   55608 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0318 17:36:43.247668   55608 cache_images.go:84] Images are preloaded, skipping loading
I0318 17:36:43.247677   55608 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0318 17:36:43.247791   55608 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0318 17:36:43.247901   55608 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0318 17:36:43.650731   55608 cni.go:84] Creating CNI manager for ""
I0318 17:36:43.650745   55608 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0318 17:36:43.650752   55608 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0318 17:36:43.650766   55608 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0318 17:36:43.650875   55608 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0318 17:36:43.650976   55608 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0318 17:36:43.704786   55608 binaries.go:44] Found k8s binaries, skipping transfer
I0318 17:36:43.704868   55608 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0318 17:36:43.745056   55608 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0318 17:36:43.792563   55608 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0318 17:36:43.851637   55608 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0318 17:36:43.893629   55608 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0318 17:36:43.910635   55608 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0318 17:36:43.953820   55608 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0318 17:36:44.171686   55608 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0318 17:36:44.266473   55608 certs.go:68] Setting up /home/avichayef/.minikube/profiles/minikube for IP: 192.168.49.2
I0318 17:36:44.266486   55608 certs.go:194] generating shared ca certs ...
I0318 17:36:44.266510   55608 certs.go:226] acquiring lock for ca certs: {Name:mkbe087a8752a5aa48d468a72e04a6ab637a8686 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:36:44.266685   55608 certs.go:235] skipping valid "minikubeCA" ca cert: /home/avichayef/.minikube/ca.key
I0318 17:36:44.266763   55608 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/avichayef/.minikube/proxy-client-ca.key
I0318 17:36:44.266771   55608 certs.go:256] generating profile certs ...
I0318 17:36:44.266842   55608 certs.go:363] generating signed profile cert for "minikube-user": /home/avichayef/.minikube/profiles/minikube/client.key
I0318 17:36:44.266874   55608 crypto.go:68] Generating cert /home/avichayef/.minikube/profiles/minikube/client.crt with IP's: []
I0318 17:36:45.059794   55608 crypto.go:156] Writing cert to /home/avichayef/.minikube/profiles/minikube/client.crt ...
I0318 17:36:45.059813   55608 lock.go:35] WriteFile acquiring /home/avichayef/.minikube/profiles/minikube/client.crt: {Name:mkfeb16e231f984a317c74eca84d6f6ba7e0fbaf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:36:45.059968   55608 crypto.go:164] Writing key to /home/avichayef/.minikube/profiles/minikube/client.key ...
I0318 17:36:45.059980   55608 lock.go:35] WriteFile acquiring /home/avichayef/.minikube/profiles/minikube/client.key: {Name:mk156ef76513191e408aed7c82c71d86324ec8d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:36:45.060169   55608 certs.go:363] generating signed profile cert for "minikube": /home/avichayef/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0318 17:36:45.060470   55608 crypto.go:68] Generating cert /home/avichayef/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0318 17:36:45.274460   55608 crypto.go:156] Writing cert to /home/avichayef/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0318 17:36:45.274473   55608 lock.go:35] WriteFile acquiring /home/avichayef/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mkb76c73545088860edb2c94d7bff68cb89897ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:36:45.274857   55608 crypto.go:164] Writing key to /home/avichayef/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0318 17:36:45.274864   55608 lock.go:35] WriteFile acquiring /home/avichayef/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk9eedb590db1abd6198fb5fdd91142ab021858e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:36:45.274989   55608 certs.go:381] copying /home/avichayef/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/avichayef/.minikube/profiles/minikube/apiserver.crt
I0318 17:36:45.275059   55608 certs.go:385] copying /home/avichayef/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/avichayef/.minikube/profiles/minikube/apiserver.key
I0318 17:36:45.275106   55608 certs.go:363] generating signed profile cert for "aggregator": /home/avichayef/.minikube/profiles/minikube/proxy-client.key
I0318 17:36:45.275115   55608 crypto.go:68] Generating cert /home/avichayef/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0318 17:36:45.391577   55608 crypto.go:156] Writing cert to /home/avichayef/.minikube/profiles/minikube/proxy-client.crt ...
I0318 17:36:45.391616   55608 lock.go:35] WriteFile acquiring /home/avichayef/.minikube/profiles/minikube/proxy-client.crt: {Name:mk7e223af373fc7ad0f0c5802a04ad0f03a58e22 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:36:45.391850   55608 crypto.go:164] Writing key to /home/avichayef/.minikube/profiles/minikube/proxy-client.key ...
I0318 17:36:45.392038   55608 lock.go:35] WriteFile acquiring /home/avichayef/.minikube/profiles/minikube/proxy-client.key: {Name:mk3a0155c36fc9f447d698dd49585eb60942efd0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:36:45.392193   55608 certs.go:484] found cert: /home/avichayef/.minikube/certs/ca-key.pem (1675 bytes)
I0318 17:36:45.392217   55608 certs.go:484] found cert: /home/avichayef/.minikube/certs/ca.pem (1086 bytes)
I0318 17:36:45.392248   55608 certs.go:484] found cert: /home/avichayef/.minikube/certs/cert.pem (1127 bytes)
I0318 17:36:45.392273   55608 certs.go:484] found cert: /home/avichayef/.minikube/certs/key.pem (1679 bytes)
I0318 17:36:45.392991   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0318 17:36:45.467555   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0318 17:36:45.548827   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0318 17:36:45.615174   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0318 17:36:45.697623   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0318 17:36:45.869373   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0318 17:36:46.213840   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0318 17:36:46.438041   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0318 17:36:46.515891   55608 ssh_runner.go:362] scp /home/avichayef/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0318 17:36:46.698608   55608 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0318 17:36:46.824799   55608 ssh_runner.go:195] Run: openssl version
I0318 17:36:46.857717   55608 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0318 17:36:46.916614   55608 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0318 17:36:46.990683   55608 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Mar 13 12:20 /usr/share/ca-certificates/minikubeCA.pem
I0318 17:36:46.990741   55608 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0318 17:36:47.031747   55608 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0318 17:36:47.064665   55608 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0318 17:36:47.082203   55608 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0318 17:36:47.082271   55608 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8192 CPUs:4 DiskSize:51200 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/avichayef:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0318 17:36:47.082407   55608 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0318 17:36:47.252255   55608 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0318 17:36:47.401575   55608 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0318 17:36:47.603618   55608 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0318 17:36:47.603685   55608 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0318 17:36:47.836644   55608 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0318 17:36:47.836650   55608 kubeadm.go:157] found existing configuration files:

I0318 17:36:47.836693   55608 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0318 17:36:48.001969   55608 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0318 17:36:48.002033   55608 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0318 17:36:48.078465   55608 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0318 17:36:48.105584   55608 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0318 17:36:48.105648   55608 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0318 17:36:48.125553   55608 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0318 17:36:48.151815   55608 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0318 17:36:48.151866   55608 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0318 17:36:48.174192   55608 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0318 17:36:48.235341   55608 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0318 17:36:48.235392   55608 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0318 17:36:48.321943   55608 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0318 17:36:48.799076   55608 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0318 17:36:49.107431   55608 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.8.0-52-generic\n", err: exit status 1
I0318 17:36:50.335736   55608 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0318 17:38:15.692308   55608 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0318 17:38:15.692343   55608 kubeadm.go:310] [preflight] Running pre-flight checks
I0318 17:38:15.692403   55608 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I0318 17:38:15.692441   55608 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.8.0-52-generic[0m
I0318 17:38:15.692464   55608 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I0318 17:38:15.692494   55608 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0318 17:38:15.692526   55608 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0318 17:38:15.692563   55608 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0318 17:38:15.692709   55608 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0318 17:38:15.692743   55608 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0318 17:38:15.692774   55608 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0318 17:38:15.692806   55608 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0318 17:38:15.692837   55608 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0318 17:38:15.692994   55608 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0318 17:38:15.693074   55608 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0318 17:38:15.693540   55608 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0318 17:38:15.693584   55608 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0318 17:38:15.786222   55608 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0318 17:38:15.807338   55608 kubeadm.go:310] [certs] Using existing ca certificate authority
I0318 17:38:15.807397   55608 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0318 17:38:15.807440   55608 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0318 17:38:15.836138   55608 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0318 17:38:15.836189   55608 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0318 17:38:15.836228   55608 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0318 17:38:15.836265   55608 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0318 17:38:15.836342   55608 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0318 17:38:15.836378   55608 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0318 17:38:15.836454   55608 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0318 17:38:15.836498   55608 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0318 17:38:15.836541   55608 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0318 17:38:15.836571   55608 kubeadm.go:310] [certs] Generating "sa" key and public key
I0318 17:38:15.864502   55608 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0318 17:38:15.864543   55608 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0318 17:38:15.864578   55608 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0318 17:38:15.864612   55608 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0318 17:38:15.864657   55608 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0318 17:38:15.864691   55608 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0318 17:38:15.864743   55608 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0318 17:38:15.864784   55608 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0318 17:38:15.980732   55608 out.go:235]     ‚ñ™ Booting up control plane ...
I0318 17:38:15.984836   55608 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0318 17:38:15.984887   55608 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0318 17:38:15.984929   55608 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0318 17:38:15.984993   55608 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0318 17:38:15.985046   55608 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0318 17:38:15.985070   55608 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0318 17:38:15.985150   55608 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0318 17:38:15.985230   55608 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0318 17:38:16.002407   55608 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.557543643s
I0318 17:38:16.002468   55608 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0318 17:38:16.002506   55608 kubeadm.go:310] [api-check] The API server is healthy after 1m8.027838424s
I0318 17:38:16.002573   55608 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0318 17:38:16.002652   55608 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0318 17:38:16.002687   55608 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0318 17:38:16.002798   55608 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0318 17:38:16.002864   55608 kubeadm.go:310] [bootstrap-token] Using token: qykuq7.kmhhk6i7nqbz60t4
I0318 17:38:16.136707   55608 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0318 17:38:16.145578   55608 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0318 17:38:16.145643   55608 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0318 17:38:16.145739   55608 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0318 17:38:16.145827   55608 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0318 17:38:16.145917   55608 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0318 17:38:16.145975   55608 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0318 17:38:16.146054   55608 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0318 17:38:16.146083   55608 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0318 17:38:16.146113   55608 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0318 17:38:16.146116   55608 kubeadm.go:310] 
I0318 17:38:16.146171   55608 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0318 17:38:16.146174   55608 kubeadm.go:310] 
I0318 17:38:16.146227   55608 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0318 17:38:16.146229   55608 kubeadm.go:310] 
I0318 17:38:16.146246   55608 kubeadm.go:310]   mkdir -p $HOME/.kube
I0318 17:38:16.146286   55608 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0318 17:38:16.146320   55608 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0318 17:38:16.146322   55608 kubeadm.go:310] 
I0318 17:38:16.146358   55608 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0318 17:38:16.146360   55608 kubeadm.go:310] 
I0318 17:38:16.146392   55608 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0318 17:38:16.146394   55608 kubeadm.go:310] 
I0318 17:38:16.146429   55608 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0318 17:38:16.146480   55608 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0318 17:38:16.146528   55608 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0318 17:38:16.146530   55608 kubeadm.go:310] 
I0318 17:38:16.146587   55608 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0318 17:38:16.146639   55608 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0318 17:38:16.146641   55608 kubeadm.go:310] 
I0318 17:38:16.146699   55608 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token qykuq7.kmhhk6i7nqbz60t4 \
I0318 17:38:16.146769   55608 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:de5a47bf753ee137b3f46354e23d7b49d8d056abf71b4c6906d05f666c49d95c \
I0318 17:38:16.146782   55608 kubeadm.go:310] 	--control-plane 
I0318 17:38:16.146784   55608 kubeadm.go:310] 
I0318 17:38:16.149936   55608 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0318 17:38:16.149946   55608 kubeadm.go:310] 
I0318 17:38:16.172933   55608 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token qykuq7.kmhhk6i7nqbz60t4 \
I0318 17:38:16.173023   55608 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:de5a47bf753ee137b3f46354e23d7b49d8d056abf71b4c6906d05f666c49d95c 
I0318 17:38:16.173028   55608 cni.go:84] Creating CNI manager for ""
I0318 17:38:16.173038   55608 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0318 17:38:16.274182   55608 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0318 17:38:16.285546   55608 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0318 17:38:16.573781   55608 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0318 17:38:17.038023   55608 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0318 17:38:17.038161   55608 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0318 17:38:17.038218   55608 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_03_18T17_38_17_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0318 17:38:21.870420   55608 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_03_18T17_38_17_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (4.832176667s)
I0318 17:38:21.870450   55608 ssh_runner.go:235] Completed: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": (4.83241737s)
I0318 17:38:21.870456   55608 ops.go:34] apiserver oom_adj: -16
I0318 17:38:21.870467   55608 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (4.832297279s)
I0318 17:38:21.870473   55608 kubeadm.go:1113] duration metric: took 4.832358287s to wait for elevateKubeSystemPrivileges
I0318 17:38:21.870478   55608 kubeadm.go:394] duration metric: took 1m34.788213535s to StartCluster
I0318 17:38:21.870489   55608 settings.go:142] acquiring lock: {Name:mk6b3941327194526af054ef27532c437eebf417 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:38:21.870543   55608 settings.go:150] Updating kubeconfig:  /home/avichayef/.kube/config
I0318 17:38:21.904493   55608 lock.go:35] WriteFile acquiring /home/avichayef/.kube/config: {Name:mk3468f78e82cd61eb4ffa888cbf52e99dc939de Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0318 17:38:21.946555   55608 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0318 17:38:21.946651   55608 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0318 17:38:21.946661   55608 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0318 17:38:21.946685   55608 host.go:66] Checking if "minikube" exists ...
I0318 17:38:21.949358   55608 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0318 17:38:21.949565   55608 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0318 17:38:21.955183   55608 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0318 17:38:21.955192   55608 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0318 17:38:21.955433   55608 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0318 17:38:21.969072   55608 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0318 17:38:22.000867   55608 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0318 17:38:22.136982   55608 out.go:177] üîé  Verifying Kubernetes components...
I0318 17:38:22.261283   55608 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0318 17:38:22.661178   55608 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0318 17:38:22.661214   55608 host.go:66] Checking if "minikube" exists ...
I0318 17:38:22.661621   55608 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0318 17:38:22.948490   55608 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0318 17:38:23.031655   55608 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0318 17:38:23.031668   55608 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0318 17:38:23.031742   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:38:23.238597   55608 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0318 17:38:23.238609   55608 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0318 17:38:23.243829   55608 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0318 17:38:23.457534   55608 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32793 SSHKeyPath:/home/avichayef/.minikube/machines/minikube/id_rsa Username:docker}
I0318 17:38:23.590869   55608 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32793 SSHKeyPath:/home/avichayef/.minikube/machines/minikube/id_rsa Username:docker}
I0318 17:38:24.915215   55608 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0318 17:38:25.022615   55608 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (2.761303249s)
I0318 17:38:25.022682   55608 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0318 17:38:25.029587   55608 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (3.080203738s)
I0318 17:38:25.029717   55608 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0318 17:38:26.202241   55608 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0318 17:38:36.631354   55608 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (11.716118179s)
I0318 17:38:36.631397   55608 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (11.601672111s)
I0318 17:38:36.631405   55608 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0318 17:38:36.637241   55608 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (11.614541919s)
I0318 17:38:36.637777   55608 api_server.go:52] waiting for apiserver process to appear ...
I0318 17:38:36.637812   55608 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0318 17:38:36.642723   55608 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (10.440462462s)
I0318 17:38:37.385630   55608 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0318 17:38:37.481224   55608 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0318 17:38:37.522475   55608 api_server.go:72] duration metric: took 15.572870417s to wait for apiserver process to appear ...
I0318 17:38:37.522491   55608 api_server.go:88] waiting for apiserver healthz status ...
I0318 17:38:37.522510   55608 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0318 17:38:37.606337   55608 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0318 17:38:37.611644   55608 api_server.go:141] control plane version: v1.32.0
I0318 17:38:37.611658   55608 api_server.go:131] duration metric: took 89.162441ms to wait for apiserver health ...
I0318 17:38:37.611663   55608 system_pods.go:43] waiting for kube-system pods to appear ...
I0318 17:38:37.624996   55608 addons.go:514] duration metric: took 15.678451541s for enable addons: enabled=[storage-provisioner default-storageclass]
I0318 17:38:37.853581   55608 system_pods.go:59] 8 kube-system pods found
I0318 17:38:37.853613   55608 system_pods.go:61] "coredns-668d6bf9bc-jhrs2" [4309d901-d142-4db1-b709-b743521f4236] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0318 17:38:37.853619   55608 system_pods.go:61] "coredns-668d6bf9bc-t9xdf" [13a092e3-566c-4372-a3be-94eeefc4006c] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0318 17:38:37.853623   55608 system_pods.go:61] "etcd-minikube" [ad33118c-b898-44c1-baef-bd8ba45e654b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0318 17:38:37.853627   55608 system_pods.go:61] "kube-apiserver-minikube" [45a7cdfd-1de6-4635-8d6c-33b48b230d26] Running
I0318 17:38:37.853630   55608 system_pods.go:61] "kube-controller-manager-minikube" [8d9a976e-7224-4c62-b5fe-bb21270185de] Running
I0318 17:38:37.853633   55608 system_pods.go:61] "kube-proxy-djt67" [c77c996f-1ed9-4f2e-825f-38e3a201ed44] Pending / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0318 17:38:37.853634   55608 system_pods.go:61] "kube-scheduler-minikube" [50a393dd-fb92-4020-a833-ae4fec4f06d7] Running
I0318 17:38:37.853638   55608 system_pods.go:61] "storage-provisioner" [1b2c06d5-4949-40e1-8121-39565e133e4b] Pending / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0318 17:38:37.853642   55608 system_pods.go:74] duration metric: took 241.975699ms to wait for pod list to return data ...
I0318 17:38:37.853650   55608 kubeadm.go:582] duration metric: took 15.904052769s to wait for: map[apiserver:true system_pods:true]
I0318 17:38:37.853658   55608 node_conditions.go:102] verifying NodePressure condition ...
I0318 17:38:38.275835   55608 node_conditions.go:122] node storage ephemeral capacity is 2112125904Ki
I0318 17:38:38.275859   55608 node_conditions.go:123] node cpu capacity is 6
I0318 17:38:38.275871   55608 node_conditions.go:105] duration metric: took 422.208978ms to run NodePressure ...
I0318 17:38:38.276160   55608 start.go:241] waiting for startup goroutines ...
I0318 17:38:38.276174   55608 start.go:246] waiting for cluster config update ...
I0318 17:38:38.276188   55608 start.go:255] writing updated cluster config ...
I0318 17:38:38.276799   55608 ssh_runner.go:195] Run: rm -f paused
I0318 17:38:40.861823   55608 start.go:600] kubectl: 1.32.3, cluster: 1.32.0 (minor skew: 0)
I0318 17:38:41.091896   55608 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 18 15:37:06 minikube cri-dockerd[1513]: time="2025-03-18T15:37:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/52f52b81ca0dba789d247637c327ca39698162569e2904a8e3d2f3e94449f361/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Mar 18 15:37:38 minikube dockerd[1241]: time="2025-03-18T15:37:38.109700318Z" level=info msg="ignoring event" container=c7c4f28d5da448f3c8cf51a33497998866c8e419b5c3f7cc89d3c7040a1f48c8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:38:29 minikube cri-dockerd[1513]: time="2025-03-18T15:38:29Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Mar 18 15:38:37 minikube cri-dockerd[1513]: time="2025-03-18T15:38:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d2ee8d93a8aeabb4eb70eb92e23e6ddc66fe14c572f8622b5245bbd0ac9ee98e/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Mar 18 15:38:40 minikube cri-dockerd[1513]: time="2025-03-18T15:38:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/caa7975cf453136ac058daeaaf0f86b30d7c39bb8e7408159b7be655412b7fe8/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Mar 18 15:38:42 minikube cri-dockerd[1513]: time="2025-03-18T15:38:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8591104a0faa8050735ea0d2ae325fdadd424993535a8a3d89937e91cef3a61a/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Mar 18 15:38:47 minikube cri-dockerd[1513]: time="2025-03-18T15:38:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e6cdbd5f7d7e35cd2da2dcca81b67766fc6ff2c14412640d8e543c61c9a8a7c2/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Mar 18 15:39:07 minikube dockerd[1241]: time="2025-03-18T15:39:07.642795758Z" level=info msg="ignoring event" container=3c21cca416d4222a17730156a2aa5358491d0c8a7a458bb9dc9dba80dce5210b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:39:09 minikube cri-dockerd[1513]: time="2025-03-18T15:39:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-t9xdf_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Mar 18 15:39:09 minikube dockerd[1241]: time="2025-03-18T15:39:09.850679286Z" level=info msg="ignoring event" container=caa7975cf453136ac058daeaaf0f86b30d7c39bb8e7408159b7be655412b7fe8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:39:27 minikube dockerd[1241]: time="2025-03-18T15:39:27.729250773Z" level=info msg="ignoring event" container=1dc881559e80082bb217eef5958901c180d069de8d9affba37118080fc396407 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:39:52 minikube cri-dockerd[1513]: time="2025-03-18T15:39:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/636afb93a2c8c4e63c9e292aef65e6c2ee0a1704ee3d062306c4668513077c07/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local home options ndots:5]"
Mar 18 15:39:53 minikube cri-dockerd[1513]: time="2025-03-18T15:39:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d24acf790632c02f64e2e6b9cf3550fcd88a3f5af783c9100b2d6d82e555e222/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local home options ndots:5]"
Mar 18 15:39:56 minikube dockerd[1241]: time="2025-03-18T15:39:56.431364210Z" level=warning msg="reference for unknown type: " digest="sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Mar 18 15:40:07 minikube cri-dockerd[1513]: time="2025-03-18T15:40:07Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=========>                                         ]  4.699MB/25.36MB"
Mar 18 15:40:17 minikube cri-dockerd[1513]: time="2025-03-18T15:40:17Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [============================================>      ]  22.77MB/25.36MB"
Mar 18 15:40:27 minikube cri-dockerd[1513]: time="2025-03-18T15:40:27Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: 4aa0ea1413d3: Extracting [==================================================>]     385B/385B"
Mar 18 15:40:37 minikube cri-dockerd[1513]: time="2025-03-18T15:40:37Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Extracting [=======>                                           ]  3.932MB/25.36MB"
Mar 18 15:40:47 minikube cri-dockerd[1513]: time="2025-03-18T15:40:47Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Extracting [================>                                  ]  8.389MB/25.36MB"
Mar 18 15:40:57 minikube cri-dockerd[1513]: time="2025-03-18T15:40:57Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Extracting [===============================>                   ]  15.73MB/25.36MB"
Mar 18 15:41:07 minikube cri-dockerd[1513]: time="2025-03-18T15:41:07Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Pull complete "
Mar 18 15:41:07 minikube cri-dockerd[1513]: time="2025-03-18T15:41:07Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Mar 18 15:41:10 minikube cri-dockerd[1513]: time="2025-03-18T15:41:10Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Mar 18 15:41:15 minikube dockerd[1241]: time="2025-03-18T15:41:15.185654252Z" level=error msg="collecting stats for container /k8s_create_ingress-nginx-admission-create-74bw8_ingress-nginx_66c4a459-cec5-43f2-b609-723a39d2cdbe_0: no metrics received"
Mar 18 15:41:15 minikube dockerd[1241]: time="2025-03-18T15:41:15.192617084Z" level=error msg="Handler for GET /v1.43/containers/f878669b656a59056a5070a7e677d6c92b161ac13080154ce91479e569816722/stats returned error: no metrics received"
Mar 18 15:41:15 minikube dockerd[1241]: 2025/03/18 15:41:15 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Mar 18 15:41:15 minikube dockerd[1241]: time="2025-03-18T15:41:15.705826811Z" level=info msg="ignoring event" container=f878669b656a59056a5070a7e677d6c92b161ac13080154ce91479e569816722 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:41:18 minikube dockerd[1241]: time="2025-03-18T15:41:18.632161810Z" level=info msg="ignoring event" container=08fc4252f7161b780cb5d77a34713740838b52d2ab56168fbcf6c33d5d00347d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:41:22 minikube dockerd[1241]: time="2025-03-18T15:41:22.463806777Z" level=info msg="ignoring event" container=636afb93a2c8c4e63c9e292aef65e6c2ee0a1704ee3d062306c4668513077c07 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:41:26 minikube dockerd[1241]: time="2025-03-18T15:41:26.173160695Z" level=info msg="ignoring event" container=d24acf790632c02f64e2e6b9cf3550fcd88a3f5af783c9100b2d6d82e555e222 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:42:00 minikube cri-dockerd[1513]: time="2025-03-18T15:42:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0db2fa797402d96af14a0f4c07ed327a450c47ebec53b39092448c653cdcde31/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local home options ndots:5]"
Mar 18 15:42:05 minikube dockerd[1241]: time="2025-03-18T15:42:05.198673682Z" level=warning msg="reference for unknown type: " digest="sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7" remote="registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"
Mar 18 15:42:16 minikube cri-dockerd[1513]: time="2025-03-18T15:42:16Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 77b70db29aee: Downloading [==============================>                    ]  520.7kB/860.9kB"
Mar 18 15:42:26 minikube cri-dockerd[1513]: time="2025-03-18T15:42:26Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 43c4264eed91: Extracting [=====================================>             ]  2.753MB/3.624MB"
Mar 18 15:42:36 minikube cri-dockerd[1513]: time="2025-03-18T15:42:36Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [===========================>                       ]  11.07MB/19.87MB"
Mar 18 15:42:46 minikube cri-dockerd[1513]: time="2025-03-18T15:42:46Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [======>                                            ]  2.611MB/20.98MB"
Mar 18 15:42:56 minikube cri-dockerd[1513]: time="2025-03-18T15:42:56Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [====================>                              ]  8.703MB/20.98MB"
Mar 18 15:43:06 minikube cri-dockerd[1513]: time="2025-03-18T15:43:06Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Extracting [=========>                                         ]  6.488MB/34.91MB"
Mar 18 15:43:16 minikube cri-dockerd[1513]: time="2025-03-18T15:43:16Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Extracting [=====================>                             ]  15.14MB/34.91MB"
Mar 18 15:43:26 minikube cri-dockerd[1513]: time="2025-03-18T15:43:26Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Extracting [===============================>                   ]  21.99MB/34.91MB"
Mar 18 15:43:36 minikube cri-dockerd[1513]: time="2025-03-18T15:43:36Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Extracting [========================================>          ]  28.48MB/34.91MB"
Mar 18 15:43:46 minikube cri-dockerd[1513]: time="2025-03-18T15:43:46Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 3adb5d6ffffa: Extracting [==>                                                ]  262.1kB/4.975MB"
Mar 18 15:43:56 minikube cri-dockerd[1513]: time="2025-03-18T15:43:56Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 3adb5d6ffffa: Extracting [==============================>                    ]   3.08MB/4.975MB"
Mar 18 15:44:06 minikube cri-dockerd[1513]: time="2025-03-18T15:44:06Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 77b70db29aee: Extracting [=========================================>         ]  720.9kB/860.9kB"
Mar 18 15:44:16 minikube cri-dockerd[1513]: time="2025-03-18T15:44:16Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Extracting [====>                                              ]  1.606MB/19.87MB"
Mar 18 15:44:26 minikube cri-dockerd[1513]: time="2025-03-18T15:44:26Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Extracting [===================>                               ]  7.569MB/19.87MB"
Mar 18 15:44:36 minikube cri-dockerd[1513]: time="2025-03-18T15:44:36Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Extracting [===================================>               ]  14.22MB/19.87MB"
Mar 18 15:44:46 minikube cri-dockerd[1513]: time="2025-03-18T15:44:46Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Extracting [============================================>      ]  17.66MB/19.87MB"
Mar 18 15:44:56 minikube cri-dockerd[1513]: time="2025-03-18T15:44:56Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 59d067fab788: Extracting [===============================================>   ]  2.982MB/3.108MB"
Mar 18 15:45:06 minikube cri-dockerd[1513]: time="2025-03-18T15:45:06Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 1fadc9c48afa: Extracting [===================>                               ]  5.571MB/14.65MB"
Mar 18 15:45:16 minikube cri-dockerd[1513]: time="2025-03-18T15:45:16Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 1fadc9c48afa: Extracting [==================================>                ]  9.994MB/14.65MB"
Mar 18 15:45:26 minikube cri-dockerd[1513]: time="2025-03-18T15:45:26Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 1fadc9c48afa: Extracting [=================================================> ]  14.42MB/14.65MB"
Mar 18 15:45:36 minikube cri-dockerd[1513]: time="2025-03-18T15:45:36Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Extracting [===========>                                       ]  4.817MB/20.98MB"
Mar 18 15:45:46 minikube cri-dockerd[1513]: time="2025-03-18T15:45:46Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Extracting [========================>                          ]  10.09MB/20.98MB"
Mar 18 15:45:56 minikube cri-dockerd[1513]: time="2025-03-18T15:45:56Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Extracting [=======================================>           ]  16.52MB/20.98MB"
Mar 18 15:46:04 minikube cri-dockerd[1513]: time="2025-03-18T15:46:04Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"
Mar 18 15:51:41 minikube dockerd[1241]: time="2025-03-18T15:51:41.076898700Z" level=info msg="Container failed to exit within 2s of signal 15 - using the force" container=c87422ce186ed8d6776f4f402cab47496fb8335df880e41eb926c0f061827ddc
Mar 18 15:51:42 minikube dockerd[1241]: time="2025-03-18T15:51:42.336095884Z" level=info msg="ignoring event" container=c87422ce186ed8d6776f4f402cab47496fb8335df880e41eb926c0f061827ddc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 18 15:51:48 minikube cri-dockerd[1513]: time="2025-03-18T15:51:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-56d7c84fd4-slhhw_ingress-nginx\": unexpected command output nsenter: cannot open /proc/4774/ns/net: No such file or directory\n with error: exit status 1"
Mar 18 15:51:48 minikube dockerd[1241]: time="2025-03-18T15:51:48.665612235Z" level=info msg="ignoring event" container=0db2fa797402d96af14a0f4c07ed327a450c47ebec53b39092448c653cdcde31 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
08fc4252f7161       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   22 minutes ago      Exited              patch                     0                   d24acf790632c       ingress-nginx-admission-patch-58hv9
f878669b656a5       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   22 minutes ago      Exited              create                    0                   636afb93a2c8c       ingress-nginx-admission-create-74bw8
c1a0da887571d       6e38f40d628db                                                                                                                24 minutes ago      Running             storage-provisioner       1                   e6cdbd5f7d7e3       storage-provisioner
1dc881559e800       6e38f40d628db                                                                                                                24 minutes ago      Exited              storage-provisioner       0                   e6cdbd5f7d7e3       storage-provisioner
6bf5cacf49acf       c69fa2e9cbf5f                                                                                                                24 minutes ago      Running             coredns                   0                   8591104a0faa8       coredns-668d6bf9bc-jhrs2
b101b0b69659b       040f9f8aac8cd                                                                                                                25 minutes ago      Running             kube-proxy                0                   d2ee8d93a8aea       kube-proxy-djt67
2bd10a6103527       8cab3d2a8bd0f                                                                                                                25 minutes ago      Running             kube-controller-manager   1                   1b86797503550       kube-controller-manager-minikube
48c21e128d024       c2e17b8d0f4a3                                                                                                                26 minutes ago      Running             kube-apiserver            0                   52f52b81ca0db       kube-apiserver-minikube
d345558f6aeef       a9e7e6b294baf                                                                                                                26 minutes ago      Running             etcd                      0                   d779f4d5bcdf9       etcd-minikube
c7c4f28d5da44       8cab3d2a8bd0f                                                                                                                26 minutes ago      Exited              kube-controller-manager   0                   1b86797503550       kube-controller-manager-minikube
ccf85dff3633b       a389e107f4ff1                                                                                                                26 minutes ago      Running             kube-scheduler            0                   643797ad6e0d6       kube-scheduler-minikube


==> coredns [6bf5cacf49ac] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[278381166]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Mar-2025 15:38:57.995) (total time: 30070ms):
Trace[278381166]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30046ms (15:39:28.042)
Trace[278381166]: [30.070458886s] [30.070458886s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[653958187]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Mar-2025 15:38:57.995) (total time: 30111ms):
Trace[653958187]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30066ms (15:39:28.062)
Trace[653958187]: [30.111586105s] [30.111586105s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[682830719]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Mar-2025 15:38:57.996) (total time: 30120ms):
Trace[682830719]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30046ms (15:39:28.042)
Trace[682830719]: [30.120294372s] [30.120294372s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] Reloading
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
[INFO] Reloading complete
[INFO] 127.0.0.1:37148 - 30242 "HINFO IN 6904092126696340424.5061584009976980093. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.10261549s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_18T17_38_17_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 18 Mar 2025 15:37:25 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 18 Mar 2025 16:03:34 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 18 Mar 2025 16:00:54 +0000   Tue, 18 Mar 2025 15:37:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 18 Mar 2025 16:00:54 +0000   Tue, 18 Mar 2025 15:37:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 18 Mar 2025 16:00:54 +0000   Tue, 18 Mar 2025 15:37:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 18 Mar 2025 16:00:54 +0000   Tue, 18 Mar 2025 15:37:26 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                6
  ephemeral-storage:  2112125904Ki
  hugepages-2Mi:      0
  memory:             17907816Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  2112125904Ki
  hugepages-2Mi:      0
  memory:             17907816Ki
  pods:               110
System Info:
  Machine ID:                 3a93af9d83c6442a93a08b0486bb8680
  System UUID:                b74d0d42-e5dc-47f0-bf38-5fd0433d4503
  Boot ID:                    f52f8e44-24a7-4236-b72c-d2e0936425d4
  Kernel Version:             6.8.0-52-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-jfj4m    100m (1%)     0 (0%)      90Mi (0%)        0 (0%)         8m50s
  kube-system                 coredns-668d6bf9bc-jhrs2                     100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     25m
  kube-system                 etcd-minikube                                100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         26m
  kube-system                 kube-apiserver-minikube                      250m (4%)     0 (0%)      0 (0%)           0 (0%)         26m
  kube-system                 kube-controller-manager-minikube             200m (3%)     0 (0%)      0 (0%)           0 (0%)         26m
  kube-system                 kube-proxy-djt67                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m
  kube-system                 kube-scheduler-minikube                      100m (1%)     0 (0%)      0 (0%)           0 (0%)         26m
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (14%)  0 (0%)
  memory             260Mi (1%)  170Mi (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 24m                kube-proxy       
  Normal  Starting                 26m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  26m (x8 over 26m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    26m (x8 over 26m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     26m (x7 over 26m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  26m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 25m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  25m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  25m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    25m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     25m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           25m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Mar18 15:00] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=16827 DF PROTO=2 
[Mar18 15:02] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=64955 DF PROTO=2 
[Mar18 15:04] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=38337 DF PROTO=2 
[Mar18 15:05] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21068 DF PROTO=TCP SPT=63427 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +0.251086] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21069 DF PROTO=TCP SPT=63428 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +0.748190] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21070 DF PROTO=TCP SPT=63427 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +0.256790] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21071 DF PROTO=TCP SPT=63428 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +1.744552] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21075 DF PROTO=TCP SPT=63427 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +0.255039] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21077 DF PROTO=TCP SPT=63428 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +3.745827] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21086 DF PROTO=TCP SPT=63427 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +0.254758] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21087 DF PROTO=TCP SPT=63428 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +7.744866] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21101 DF PROTO=TCP SPT=63427 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[  +0.256211] [UFW BLOCK] IN=enp0s3 OUT= MAC=08:00:27:94:b9:7a:18:60:24:88:00:86:08:00 SRC=10.100.102.8 DST=10.100.102.50 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=21102 DF PROTO=TCP SPT=63428 DPT=5000 WINDOW=64240 RES=0x00 SYN URGP=0 
[Mar18 15:06] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=43919 DF PROTO=2 
[Mar18 15:08] workqueue: addrconf_dad_work hogged CPU for >10000us 8 times, consider switching to WQ_UNBOUND
[ +27.013244] workqueue: process_srcu hogged CPU for >10000us 32 times, consider switching to WQ_UNBOUND
[ +14.459692] workqueue: srcu_invoke_callbacks hogged CPU for >10000us 64 times, consider switching to WQ_UNBOUND
[Mar18 15:09] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=63149 DF PROTO=2 
[Mar18 15:11] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=29258 DF PROTO=2 
[ +22.573668] workqueue: kernfs_notify_workfn hogged CPU for >10000us 8 times, consider switching to WQ_UNBOUND
[ +26.037988] workqueue: wait_rcu_exp_gp hogged CPU for >10000us 1024 times, consider switching to WQ_UNBOUND
[Mar18 15:12] workqueue: e1000_watchdog [e1000] hogged CPU for >10000us 2048 times, consider switching to WQ_UNBOUND
[Mar18 15:13] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=35205 DF PROTO=2 
[Mar18 15:15] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=25880 DF PROTO=2 
[ +35.241612] workqueue: free_ipc hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +5.584170] workqueue: fqdir_free_fn hogged CPU for >10000us 32 times, consider switching to WQ_UNBOUND
[Mar18 15:17] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=56126 DF PROTO=2 
[Mar18 15:19] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=34045 DF PROTO=2 
[Mar18 15:20] workqueue: blk_mq_requeue_work hogged CPU for >10000us 2048 times, consider switching to WQ_UNBOUND
[Mar18 15:21] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=39520 DF PROTO=2 
[Mar18 15:23] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=12868 DF PROTO=2 
[Mar18 15:25] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=63652 DF PROTO=2 
[Mar18 15:26] workqueue: vmstat_update hogged CPU for >10000us 32 times, consider switching to WQ_UNBOUND
[Mar18 15:27] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=16668 DF PROTO=2 
[Mar18 15:29] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=27870 DF PROTO=2 
[  +6.365520] workqueue: kernfs_notify_workfn hogged CPU for >10000us 16 times, consider switching to WQ_UNBOUND
[Mar18 15:31] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=42131 DF PROTO=2 
[Mar18 15:33] workqueue: css_release_work_fn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[Mar18 15:34] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=53354 DF PROTO=2 
[Mar18 15:36] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=44788 DF PROTO=2 
[Mar18 15:37] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 2048 times, consider switching to WQ_UNBOUND
[Mar18 15:38] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=18525 DF PROTO=2 
[Mar18 15:40] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=45191 DF PROTO=2 
[Mar18 15:41] workqueue: netstamp_clear hogged CPU for >10000us 16 times, consider switching to WQ_UNBOUND
[Mar18 15:42] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=27727 DF PROTO=2 
[Mar18 15:44] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=30238 DF PROTO=2 
[Mar18 15:46] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=21709 DF PROTO=2 
[Mar18 15:48] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=27499 DF PROTO=2 
[Mar18 15:50] workqueue: ata_sff_pio_task hogged CPU for >10000us 256 times, consider switching to WQ_UNBOUND
[ +24.676419] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=60045 DF PROTO=2 
[Mar18 15:51] workqueue: srcu_invoke_callbacks hogged CPU for >10000us 128 times, consider switching to WQ_UNBOUND
[  +6.374480] workqueue: process_srcu hogged CPU for >10000us 64 times, consider switching to WQ_UNBOUND
[Mar18 15:52] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=10854 DF PROTO=2 
[Mar18 15:54] workqueue: drm_fb_helper_damage_work hogged CPU for >10000us 1024 times, consider switching to WQ_UNBOUND
[ +21.371168] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=59798 DF PROTO=2 
[Mar18 15:56] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=53688 DF PROTO=2 
[Mar18 15:59] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=36715 DF PROTO=2 
[ +51.738511] workqueue: vmstat_shepherd hogged CPU for >10000us 1024 times, consider switching to WQ_UNBOUND
[Mar18 16:01] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=11691 DF PROTO=2 
[Mar18 16:03] [UFW BLOCK] IN=enp0s3 OUT= MAC=01:00:5e:00:00:01:50:6f:0c:05:7c:07:08:00 SRC=10.100.102.1 DST=224.0.0.1 LEN=36 TOS=0x00 PREC=0x00 TTL=1 ID=38532 DF PROTO=2 


==> etcd [d345558f6aee] <==
{"level":"warn","ts":"2025-03-18T15:58:41.322764Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"131.250698ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035984452411169 > lease_revoke:<id:70cc95a9e6e0deb3>","response":"size:29"}
{"level":"info","ts":"2025-03-18T15:58:41.322885Z","caller":"traceutil/trace.go:171","msg":"trace[1458606525] linearizableReadLoop","detail":"{readStateIndex:1905; appliedIndex:1904; }","duration":"133.907747ms","start":"2025-03-18T15:58:41.188965Z","end":"2025-03-18T15:58:41.322873Z","steps":["trace[1458606525] 'read index received'  (duration: 47.606394ms)","trace[1458606525] 'applied index is now lower than readState.Index'  (duration: 86.300363ms)"],"step_count":2}
{"level":"warn","ts":"2025-03-18T15:58:41.322958Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"133.987274ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-18T15:58:41.322972Z","caller":"traceutil/trace.go:171","msg":"trace[2123028487] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1634; }","duration":"134.008934ms","start":"2025-03-18T15:58:41.188957Z","end":"2025-03-18T15:58:41.322966Z","steps":["trace[2123028487] 'agreement among raft nodes before linearized reading'  (duration: 133.97766ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T15:58:44.285595Z","caller":"traceutil/trace.go:171","msg":"trace[1833697557] transaction","detail":"{read_only:false; response_revision:1636; number_of_response:1; }","duration":"101.687653ms","start":"2025-03-18T15:58:44.183890Z","end":"2025-03-18T15:58:44.285578Z","steps":["trace[1833697557] 'process raft request'  (duration: 101.576161ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T15:58:44.305627Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.064783ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:704"}
{"level":"info","ts":"2025-03-18T15:58:44.314814Z","caller":"traceutil/trace.go:171","msg":"trace[1905790622] range","detail":"{range_begin:/registry/services/specs/default/kubernetes; range_end:; response_count:1; response_revision:1636; }","duration":"130.651408ms","start":"2025-03-18T15:58:44.184140Z","end":"2025-03-18T15:58:44.314791Z","steps":["trace[1905790622] 'agreement among raft nodes before linearized reading'  (duration: 121.363426ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T15:58:44.300990Z","caller":"traceutil/trace.go:171","msg":"trace[877297737] linearizableReadLoop","detail":"{readStateIndex:1907; appliedIndex:1907; }","duration":"116.432696ms","start":"2025-03-18T15:58:44.184542Z","end":"2025-03-18T15:58:44.300975Z","steps":["trace[877297737] 'read index received'  (duration: 116.428173ms)","trace[877297737] 'applied index is now lower than readState.Index'  (duration: 3.815¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-03-18T15:58:44.315814Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"129.56429ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-18T15:58:44.315875Z","caller":"traceutil/trace.go:171","msg":"trace[1579976563] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1636; }","duration":"129.66486ms","start":"2025-03-18T15:58:44.186184Z","end":"2025-03-18T15:58:44.315849Z","steps":["trace[1579976563] 'agreement among raft nodes before linearized reading'  (duration: 129.544724ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T15:58:49.101400Z","caller":"traceutil/trace.go:171","msg":"trace[210278066] transaction","detail":"{read_only:false; response_revision:1640; number_of_response:1; }","duration":"112.614152ms","start":"2025-03-18T15:58:48.988772Z","end":"2025-03-18T15:58:49.101386Z","steps":["trace[210278066] 'process raft request'  (duration: 110.036841ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T15:58:56.084290Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.65179ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-18T15:58:56.130513Z","caller":"traceutil/trace.go:171","msg":"trace[402481330] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:1644; }","duration":"154.467294ms","start":"2025-03-18T15:58:55.976030Z","end":"2025-03-18T15:58:56.130497Z","steps":["trace[402481330] 'range keys from in-memory index tree'  (duration: 104.554985ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T15:58:56.125344Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"232.706949ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-03-18T15:58:56.130673Z","caller":"traceutil/trace.go:171","msg":"trace[1637809288] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:1644; }","duration":"238.052055ms","start":"2025-03-18T15:58:55.892616Z","end":"2025-03-18T15:58:56.130668Z","steps":["trace[1637809288] 'range keys from in-memory index tree'  (duration: 229.600251ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T15:58:56.183187Z","caller":"traceutil/trace.go:171","msg":"trace[1723800418] transaction","detail":"{read_only:false; response_revision:1645; number_of_response:1; }","duration":"102.784238ms","start":"2025-03-18T15:58:56.080393Z","end":"2025-03-18T15:58:56.183177Z","steps":["trace[1723800418] 'process raft request'  (duration: 101.896158ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T15:58:56.536527Z","caller":"traceutil/trace.go:171","msg":"trace[2134324172] transaction","detail":"{read_only:false; response_revision:1646; number_of_response:1; }","duration":"100.01934ms","start":"2025-03-18T15:58:56.436495Z","end":"2025-03-18T15:58:56.536514Z","steps":["trace[2134324172] 'process raft request'  (duration: 81.544135ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T15:59:06.901057Z","caller":"traceutil/trace.go:171","msg":"trace[577557069] transaction","detail":"{read_only:false; response_revision:1655; number_of_response:1; }","duration":"109.225455ms","start":"2025-03-18T15:59:06.791820Z","end":"2025-03-18T15:59:06.901046Z","steps":["trace[577557069] 'process raft request'  (duration: 109.152189ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T15:59:16.201149Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"134.214552ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2025-03-18T15:59:16.212353Z","caller":"traceutil/trace.go:171","msg":"trace[1941891509] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:1661; }","duration":"145.440288ms","start":"2025-03-18T15:59:16.066889Z","end":"2025-03-18T15:59:16.212330Z","steps":["trace[1941891509] 'range keys from in-memory index tree'  (duration: 134.150111ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T15:59:47.976182Z","caller":"traceutil/trace.go:171","msg":"trace[2075485057] transaction","detail":"{read_only:false; response_revision:1685; number_of_response:1; }","duration":"118.515871ms","start":"2025-03-18T15:59:47.857654Z","end":"2025-03-18T15:59:47.976170Z","steps":["trace[2075485057] 'process raft request'  (duration: 118.443644ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T15:59:52.650653Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.789062ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-18T15:59:52.650740Z","caller":"traceutil/trace.go:171","msg":"trace[731761939] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:1689; }","duration":"112.940952ms","start":"2025-03-18T15:59:52.537784Z","end":"2025-03-18T15:59:52.650725Z","steps":["trace[731761939] 'range keys from in-memory index tree'  (duration: 112.711485ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T16:00:01.555878Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.790712ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035984452411722 > lease_revoke:<id:70cc95a9e6e0e0e1>","response":"size:29"}
{"level":"info","ts":"2025-03-18T16:00:09.635872Z","caller":"traceutil/trace.go:171","msg":"trace[398682600] transaction","detail":"{read_only:false; response_revision:1702; number_of_response:1; }","duration":"106.911332ms","start":"2025-03-18T16:00:09.528945Z","end":"2025-03-18T16:00:09.635857Z","steps":["trace[398682600] 'process raft request'  (duration: 106.819197ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:00:14.066073Z","caller":"traceutil/trace.go:171","msg":"trace[1468237522] transaction","detail":"{read_only:false; response_revision:1705; number_of_response:1; }","duration":"119.802879ms","start":"2025-03-18T16:00:13.946258Z","end":"2025-03-18T16:00:14.066061Z","steps":["trace[1468237522] 'process raft request'  (duration: 119.731992ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:00:29.859076Z","caller":"traceutil/trace.go:171","msg":"trace[1381368522] transaction","detail":"{read_only:false; response_revision:1717; number_of_response:1; }","duration":"105.044007ms","start":"2025-03-18T16:00:29.754013Z","end":"2025-03-18T16:00:29.859057Z","steps":["trace[1381368522] 'process raft request'  (duration: 104.934734ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:00:54.510269Z","caller":"traceutil/trace.go:171","msg":"trace[793668282] transaction","detail":"{read_only:false; response_revision:1736; number_of_response:1; }","duration":"103.689327ms","start":"2025-03-18T16:00:54.406567Z","end":"2025-03-18T16:00:54.510257Z","steps":["trace[793668282] 'process raft request'  (duration: 103.602916ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:00:58.337415Z","caller":"traceutil/trace.go:171","msg":"trace[146361325] transaction","detail":"{read_only:false; response_revision:1739; number_of_response:1; }","duration":"106.630538ms","start":"2025-03-18T16:00:58.230773Z","end":"2025-03-18T16:00:58.337403Z","steps":["trace[146361325] 'process raft request'  (duration: 106.448378ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:01:03.802026Z","caller":"traceutil/trace.go:171","msg":"trace[1026567626] transaction","detail":"{read_only:false; response_revision:1743; number_of_response:1; }","duration":"110.132582ms","start":"2025-03-18T16:01:03.691866Z","end":"2025-03-18T16:01:03.801998Z","steps":["trace[1026567626] 'process raft request'  (duration: 97.868457ms)","trace[1026567626] 'store kv pair into bolt db' {req_type:put; key:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; req_size:670; } (duration: 12.153282ms)"],"step_count":2}
{"level":"warn","ts":"2025-03-18T16:01:06.320852Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.681185ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035984452412137 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1737 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128035984452412134 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-03-18T16:01:06.365029Z","caller":"traceutil/trace.go:171","msg":"trace[2930331] transaction","detail":"{read_only:false; response_revision:1746; number_of_response:1; }","duration":"243.831508ms","start":"2025-03-18T16:01:06.121163Z","end":"2025-03-18T16:01:06.364995Z","steps":["trace[2930331] 'process raft request'  (duration: 90.967588ms)","trace[2930331] 'compare'  (duration: 108.584521ms)"],"step_count":2}
{"level":"info","ts":"2025-03-18T16:01:26.171342Z","caller":"traceutil/trace.go:171","msg":"trace[145492822] transaction","detail":"{read_only:false; response_revision:1761; number_of_response:1; }","duration":"102.875074ms","start":"2025-03-18T16:01:26.068455Z","end":"2025-03-18T16:01:26.171330Z","steps":["trace[145492822] 'process raft request'  (duration: 102.794011ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:01:35.412457Z","caller":"traceutil/trace.go:171","msg":"trace[1319541441] transaction","detail":"{read_only:false; response_revision:1768; number_of_response:1; }","duration":"134.122116ms","start":"2025-03-18T16:01:35.278315Z","end":"2025-03-18T16:01:35.412438Z","steps":["trace[1319541441] 'process raft request'  (duration: 134.013149ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T16:01:41.258651Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.356646ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035984452412310 > lease_revoke:<id:70cc95a9e6e0e341>","response":"size:29"}
{"level":"info","ts":"2025-03-18T16:01:50.437276Z","caller":"traceutil/trace.go:171","msg":"trace[1853434353] transaction","detail":"{read_only:false; response_revision:1779; number_of_response:1; }","duration":"111.695689ms","start":"2025-03-18T16:01:50.325568Z","end":"2025-03-18T16:01:50.437264Z","steps":["trace[1853434353] 'process raft request'  (duration: 111.624179ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T16:01:51.424936Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"133.111469ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035984452412360 > lease_revoke:<id:70cc95a9e6e0e37b>","response":"size:29"}
{"level":"info","ts":"2025-03-18T16:02:01.716662Z","caller":"traceutil/trace.go:171","msg":"trace[542893399] transaction","detail":"{read_only:false; response_revision:1787; number_of_response:1; }","duration":"106.579772ms","start":"2025-03-18T16:02:01.610070Z","end":"2025-03-18T16:02:01.716650Z","steps":["trace[542893399] 'process raft request'  (duration: 106.489968ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:02:19.180393Z","caller":"traceutil/trace.go:171","msg":"trace[2111227326] transaction","detail":"{read_only:false; response_revision:1802; number_of_response:1; }","duration":"180.332848ms","start":"2025-03-18T16:02:19.000017Z","end":"2025-03-18T16:02:19.180350Z","steps":["trace[2111227326] 'process raft request'  (duration: 120.025582ms)","trace[2111227326] 'compare'  (duration: 52.43503ms)"],"step_count":2}
{"level":"info","ts":"2025-03-18T16:02:19.209600Z","caller":"traceutil/trace.go:171","msg":"trace[1931580631] compact","detail":"{revision:1573; response_revision:1802; }","duration":"151.620059ms","start":"2025-03-18T16:02:19.057969Z","end":"2025-03-18T16:02:19.209589Z","steps":["trace[1931580631] 'process raft request'  (duration: 114.624017ms)","trace[1931580631] 'check and update compact revision'  (duration: 35.915876ms)"],"step_count":2}
{"level":"info","ts":"2025-03-18T16:02:19.209057Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1573}
{"level":"info","ts":"2025-03-18T16:02:19.360579Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1573,"took":"150.404237ms","hash":1349108256,"current-db-size-bytes":2748416,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1945600,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-03-18T16:02:19.365410Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1349108256,"revision":1573,"compact-revision":1290}
{"level":"info","ts":"2025-03-18T16:02:25.905008Z","caller":"traceutil/trace.go:171","msg":"trace[1623817705] transaction","detail":"{read_only:false; response_revision:1806; number_of_response:1; }","duration":"154.618156ms","start":"2025-03-18T16:02:25.750378Z","end":"2025-03-18T16:02:25.904996Z","steps":["trace[1623817705] 'process raft request'  (duration: 154.543382ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:02:26.341798Z","caller":"traceutil/trace.go:171","msg":"trace[1572789422] transaction","detail":"{read_only:false; response_revision:1808; number_of_response:1; }","duration":"106.969534ms","start":"2025-03-18T16:02:26.234817Z","end":"2025-03-18T16:02:26.341787Z","steps":["trace[1572789422] 'process raft request'  (duration: 72.50846ms)","trace[1572789422] 'compare'  (duration: 34.363161ms)"],"step_count":2}
{"level":"info","ts":"2025-03-18T16:02:34.558199Z","caller":"traceutil/trace.go:171","msg":"trace[1750914482] transaction","detail":"{read_only:false; response_revision:1813; number_of_response:1; }","duration":"103.712321ms","start":"2025-03-18T16:02:34.454475Z","end":"2025-03-18T16:02:34.558187Z","steps":["trace[1750914482] 'process raft request'  (duration: 103.595522ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T16:02:34.766183Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.472648ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas/\" range_end:\"/registry/resourcequotas0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-03-18T16:02:34.768277Z","caller":"traceutil/trace.go:171","msg":"trace[1651966536] range","detail":"{range_begin:/registry/resourcequotas/; range_end:/registry/resourcequotas0; response_count:0; response_revision:1813; }","duration":"102.590321ms","start":"2025-03-18T16:02:34.665674Z","end":"2025-03-18T16:02:34.768264Z","steps":["trace[1651966536] 'count revisions from in-memory index tree'  (duration: 100.411928ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:02:36.310344Z","caller":"traceutil/trace.go:171","msg":"trace[171947590] transaction","detail":"{read_only:false; response_revision:1815; number_of_response:1; }","duration":"107.605375ms","start":"2025-03-18T16:02:36.202721Z","end":"2025-03-18T16:02:36.310327Z","steps":["trace[171947590] 'process raft request'  (duration: 51.253116ms)","trace[171947590] 'compare'  (duration: 39.382819ms)","trace[171947590] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:115; } (duration: 16.700233ms)"],"step_count":3}
{"level":"info","ts":"2025-03-18T16:02:53.813956Z","caller":"traceutil/trace.go:171","msg":"trace[1187322530] transaction","detail":"{read_only:false; response_revision:1828; number_of_response:1; }","duration":"101.247155ms","start":"2025-03-18T16:02:53.712695Z","end":"2025-03-18T16:02:53.813942Z","steps":["trace[1187322530] 'process raft request'  (duration: 101.167331ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T16:02:56.604234Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"139.633023ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:133"}
{"level":"info","ts":"2025-03-18T16:02:56.604276Z","caller":"traceutil/trace.go:171","msg":"trace[1796631298] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:1830; }","duration":"139.703531ms","start":"2025-03-18T16:02:56.464564Z","end":"2025-03-18T16:02:56.604268Z","steps":["trace[1796631298] 'range keys from in-memory index tree'  (duration: 139.584275ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:02:58.402777Z","caller":"traceutil/trace.go:171","msg":"trace[1210309857] transaction","detail":"{read_only:false; response_revision:1832; number_of_response:1; }","duration":"128.37423ms","start":"2025-03-18T16:02:58.274391Z","end":"2025-03-18T16:02:58.402765Z","steps":["trace[1210309857] 'process raft request'  (duration: 127.812146ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T16:03:24.614647Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"150.062974ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035984452412802 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1850 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-03-18T16:03:24.619221Z","caller":"traceutil/trace.go:171","msg":"trace[461753692] transaction","detail":"{read_only:false; response_revision:1852; number_of_response:1; }","duration":"188.521209ms","start":"2025-03-18T16:03:24.430686Z","end":"2025-03-18T16:03:24.619207Z","steps":["trace[461753692] 'process raft request'  (duration: 33.851856ms)","trace[461753692] 'compare'  (duration: 149.866299ms)"],"step_count":2}
{"level":"info","ts":"2025-03-18T16:03:31.504341Z","caller":"traceutil/trace.go:171","msg":"trace[213486699] transaction","detail":"{read_only:false; response_revision:1857; number_of_response:1; }","duration":"128.653138ms","start":"2025-03-18T16:03:31.375675Z","end":"2025-03-18T16:03:31.504328Z","steps":["trace[213486699] 'process raft request'  (duration: 128.576401ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:03:36.472333Z","caller":"traceutil/trace.go:171","msg":"trace[1678295829] transaction","detail":"{read_only:false; response_revision:1860; number_of_response:1; }","duration":"169.564132ms","start":"2025-03-18T16:03:36.302752Z","end":"2025-03-18T16:03:36.472316Z","steps":["trace[1678295829] 'process raft request'  (duration: 169.44283ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:03:36.513641Z","caller":"traceutil/trace.go:171","msg":"trace[1945665589] transaction","detail":"{read_only:false; response_revision:1861; number_of_response:1; }","duration":"189.367948ms","start":"2025-03-18T16:03:36.324256Z","end":"2025-03-18T16:03:36.513624Z","steps":["trace[1945665589] 'process raft request'  (duration: 189.272512ms)"],"step_count":1}
{"level":"info","ts":"2025-03-18T16:03:41.041492Z","caller":"traceutil/trace.go:171","msg":"trace[867294855] transaction","detail":"{read_only:false; response_revision:1864; number_of_response:1; }","duration":"111.183068ms","start":"2025-03-18T16:03:40.930297Z","end":"2025-03-18T16:03:41.041480Z","steps":["trace[867294855] 'process raft request'  (duration: 111.10739ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-18T16:03:41.961814Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"139.725095ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035984452412882 > lease_revoke:<id:70cc95a9e6e0e589>","response":"size:29"}


==> kernel <==
 16:03:44 up  4:17,  0 users,  load average: 6.10, 5.45, 7.45
Linux minikube 6.8.0-52-generic #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [48c21e128d02] <==
I0318 15:37:25.420213       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0318 15:37:25.420685       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0318 15:37:25.420700       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0318 15:37:25.420756       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0318 15:37:25.420765       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0318 15:37:25.434885       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0318 15:37:25.449110       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0318 15:37:25.449163       1 controller.go:142] Starting OpenAPI controller
I0318 15:37:25.449177       1 controller.go:90] Starting OpenAPI V3 controller
I0318 15:37:25.449186       1 naming_controller.go:294] Starting NamingConditionController
I0318 15:37:25.449195       1 establishing_controller.go:81] Starting EstablishingController
I0318 15:37:25.449204       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0318 15:37:25.449209       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0318 15:37:25.449214       1 crd_finalizer.go:269] Starting CRDFinalizer
E0318 15:37:25.659724       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0318 15:37:25.662276       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0318 15:37:25.671272       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0318 15:37:25.672695       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="12.936025ms" method="GET" path="/healthz" result=null
I0318 15:37:25.719881       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0318 15:37:25.726136       1 cache.go:39] Caches are synced for LocalAvailability controller
I0318 15:37:25.729931       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0318 15:37:25.745646       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0318 15:37:25.751056       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0318 15:37:25.773036       1 aggregator.go:171] initial CRD sync complete...
I0318 15:37:25.773045       1 autoregister_controller.go:144] Starting autoregister controller
I0318 15:37:25.773053       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0318 15:37:25.773062       1 cache.go:39] Caches are synced for autoregister controller
I0318 15:37:25.829369       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0318 15:37:25.829424       1 shared_informer.go:320] Caches are synced for configmaps
I0318 15:37:25.858333       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0318 15:37:25.858392       1 policy_source.go:240] refreshing policies
I0318 15:37:25.858512       1 shared_informer.go:320] Caches are synced for node_authorizer
I0318 15:37:25.858632       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0318 15:37:25.863709       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0318 15:37:25.945893       1 controller.go:615] quota admission added evaluator for: namespaces
E0318 15:37:26.553381       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
E0318 15:37:26.562547       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0318 15:37:26.776824       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0318 15:37:27.242516       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0318 15:37:27.839871       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0318 15:37:27.845712       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0318 15:38:01.733037       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0318 15:38:03.814690       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0318 15:38:05.119966       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0318 15:38:05.519138       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0318 15:38:05.561044       1 controller.go:615] quota admission added evaluator for: endpoints
I0318 15:38:06.082298       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0318 15:38:06.231040       1 controller.go:615] quota admission added evaluator for: serviceaccounts
E0318 15:38:12.021127       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="1.807855ms" method="GET" path="/readyz" result=null
I0318 15:38:13.127870       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0318 15:38:13.761367       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0318 15:38:14.272410       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0318 15:38:23.427030       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0318 15:38:24.691214       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0318 15:39:40.329320       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.102.252.42"}
I0318 15:39:41.067846       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.111.243.198"}
I0318 15:39:41.634793       1 controller.go:615] quota admission added evaluator for: jobs.batch
E0318 15:39:42.158981       1 queueset.go:474] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2025-03-18 15:39:42.158951707" prevR="25.92432328ss" incrR="184467440737.09549597ss" currentR="25.92430309ss"
E0318 15:51:39.108666       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has been invalidated]"
E0318 15:51:41.346859       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has been invalidated]"


==> kube-controller-manager [2bd10a610352] <==
I0318 15:38:30.237187       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 15:38:38.603914       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="1.205635321s"
I0318 15:38:39.521737       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="917.776814ms"
I0318 15:38:39.521843       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="74.604¬µs"
I0318 15:39:00.242237       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="35.273¬µs"
I0318 15:39:01.454069       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="45.832¬µs"
I0318 15:39:12.252907       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="38.418¬µs"
I0318 15:39:12.793172       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="39.114¬µs"
I0318 15:39:13.193403       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="39.894¬µs"
I0318 15:39:13.456488       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="40.571¬µs"
I0318 15:39:34.055582       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="351.21062ms"
I0318 15:39:34.082008       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="60.706¬µs"
I0318 15:39:41.674950       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I0318 15:39:42.213249       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="646.801408ms"
I0318 15:39:42.298140       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0318 15:39:42.394246       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
I0318 15:39:42.541599       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="314.002445ms"
I0318 15:39:42.546220       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="67.171¬µs"
I0318 15:39:42.695667       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0318 15:39:42.765983       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0318 15:39:42.833750       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:39:42.844750       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="61.906¬µs"
I0318 15:39:42.928856       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:39:42.933601       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:39:43.280623       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0318 15:39:44.257021       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:41:17.724792       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0318 15:41:18.220890       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:41:19.525948       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:41:20.413599       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:41:20.669514       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:41:24.941039       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0318 15:41:25.217318       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 15:41:26.172995       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0318 15:41:26.537675       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0318 15:41:28.255772       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:41:29.319400       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:41:29.593557       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0318 15:44:59.898726       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 15:46:12.055392       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="13.93648ms"
I0318 15:46:21.829454       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 15:46:24.926146       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="461.100627ms"
I0318 15:46:24.958393       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="105.107¬µs"
I0318 15:50:38.626321       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 15:51:37.121462       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="7.79¬µs"
I0318 15:51:38.629656       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="ingress-nginx/ingress-nginx-controller" err="EndpointSlice informer cache is out of date"
I0318 15:52:05.082210       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="631.585179ms"
I0318 15:52:05.637082       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="541.435102ms"
I0318 15:52:05.650194       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="35.317¬µs"
I0318 15:54:53.357049       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="1.012671115s"
I0318 15:54:53.853621       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="815.756605ms"
I0318 15:54:54.155648       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="297.038845ms"
I0318 15:54:54.190473       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="63.476¬µs"
I0318 15:54:54.280686       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="923.603659ms"
I0318 15:54:54.280904       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="90.433¬µs"
I0318 15:54:55.519023       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="44.394¬µs"
I0318 15:54:55.583417       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="34.783¬µs"
I0318 15:54:55.646068       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-66cb9865b5" duration="37.364¬µs"
I0318 15:55:45.985044       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0318 16:00:54.573258       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [c7c4f28d5da4] <==
I0318 15:37:16.958372       1 serving.go:386] Generated self-signed cert in-memory
I0318 15:37:18.122051       1 controllermanager.go:185] "Starting" version="v1.32.0"
I0318 15:37:18.186543       1 controllermanager.go:187] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0318 15:37:18.290311       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0318 15:37:18.305836       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0318 15:37:18.309329       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0318 15:37:18.309654       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0318 15:37:35.792459       1 controllermanager.go:230] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: forbidden: User \"system:kube-controller-manager\" cannot get path \"/healthz\""


==> kube-proxy [b101b0b69659] <==
I0318 15:38:51.173516       1 server_linux.go:66] "Using iptables proxy"
I0318 15:38:52.863340       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0318 15:38:52.863441       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0318 15:38:54.900426       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0318 15:38:54.900553       1 server_linux.go:170] "Using iptables Proxier"
I0318 15:38:55.146526       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0318 15:38:55.481626       1 server.go:497] "Version info" version="v1.32.0"
I0318 15:38:55.481657       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0318 15:38:55.521817       1 config.go:199] "Starting service config controller"
I0318 15:38:55.521850       1 shared_informer.go:313] Waiting for caches to sync for service config
I0318 15:38:55.521896       1 config.go:105] "Starting endpoint slice config controller"
I0318 15:38:55.521900       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0318 15:38:55.522375       1 config.go:329] "Starting node config controller"
I0318 15:38:55.522381       1 shared_informer.go:313] Waiting for caches to sync for node config
I0318 15:38:55.625199       1 shared_informer.go:320] Caches are synced for node config
I0318 15:38:55.626269       1 shared_informer.go:320] Caches are synced for service config
I0318 15:38:55.722024       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [ccf85dff3633] <==
E0318 15:37:34.415861       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:34.506403       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0318 15:37:34.517285       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0318 15:37:34.533620       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0318 15:37:34.582524       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:34.533657       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0318 15:37:34.608423       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:34.568833       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:34.608444       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:34.655942       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0318 15:37:34.749199       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:34.749259       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0318 15:37:34.749315       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0318 15:37:34.807844       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0318 15:37:34.812903       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:35.035790       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0318 15:37:35.064604       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0318 15:37:35.228482       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:35.228729       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:35.779712       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0318 15:37:35.779743       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:36.343451       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:36.343481       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:36.553562       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0318 15:37:36.594493       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:37.128345       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0318 15:37:37.128388       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0318 15:37:43.834089       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0318 15:37:43.834133       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0318 15:37:44.526037       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0318 15:37:44.526271       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:44.761972       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:44.762103       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:45.030196       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:45.040699       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:45.100196       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0318 15:37:45.100221       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:45.199579       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0318 15:37:45.199613       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:45.433587       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0318 15:37:45.434654       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:45.912222       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0318 15:37:45.912252       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0318 15:37:45.912290       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0318 15:37:45.912301       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0318 15:37:46.082474       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0318 15:37:46.110228       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0318 15:37:46.843630       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0318 15:37:46.843660       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:47.100784       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:47.100933       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:48.038287       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:48.038316       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:48.104240       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0318 15:37:48.104378       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0318 15:37:48.494511       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0318 15:37:48.494542       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0318 15:37:49.163751       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0318 15:37:49.163876       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0318 15:38:06.942401       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Mar 18 15:52:05 minikube kubelet[2513]: I0318 15:52:05.109947    2513 memory_manager.go:355] "RemoveStaleState removing state" podUID="66c4a459-cec5-43f2-b609-723a39d2cdbe" containerName="create"
Mar 18 15:52:05 minikube kubelet[2513]: I0318 15:52:05.109968    2513 memory_manager.go:355] "RemoveStaleState removing state" podUID="ad49d79b-58e3-4321-a9c6-702e9d838079" containerName="patch"
Mar 18 15:52:05 minikube kubelet[2513]: I0318 15:52:05.109973    2513 memory_manager.go:355] "RemoveStaleState removing state" podUID="ea4d8efa-aab2-46f2-9aac-5c579727b9c5" containerName="controller"
Mar 18 15:52:05 minikube kubelet[2513]: I0318 15:52:05.263783    2513 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qc8cm\" (UniqueName: \"kubernetes.io/projected/690702d6-99e8-43c7-9840-18af74ff4160-kube-api-access-qc8cm\") pod \"ingress-nginx-controller-66cb9865b5-blpxj\" (UID: \"690702d6-99e8-43c7-9840-18af74ff4160\") " pod="ingress-nginx/ingress-nginx-controller-66cb9865b5-blpxj"
Mar 18 15:52:05 minikube kubelet[2513]: I0318 15:52:05.307361    2513 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert\") pod \"ingress-nginx-controller-66cb9865b5-blpxj\" (UID: \"690702d6-99e8-43c7-9840-18af74ff4160\") " pod="ingress-nginx/ingress-nginx-controller-66cb9865b5-blpxj"
Mar 18 15:52:05 minikube kubelet[2513]: E0318 15:52:05.437570    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:52:05 minikube kubelet[2513]: E0318 15:52:05.442533    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:52:05.942516328 +0000 UTC m=+833.715485721 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:52:06 minikube kubelet[2513]: E0318 15:52:05.998254    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:52:06 minikube kubelet[2513]: E0318 15:52:05.998690    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:52:06.998670508 +0000 UTC m=+834.771639914 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:52:07 minikube kubelet[2513]: E0318 15:52:07.062602    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:52:07 minikube kubelet[2513]: E0318 15:52:07.062685    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:52:09.062665253 +0000 UTC m=+836.835634660 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:52:09 minikube kubelet[2513]: E0318 15:52:09.154794    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:52:09 minikube kubelet[2513]: E0318 15:52:09.159909    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:52:13.159887792 +0000 UTC m=+840.932857187 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:52:13 minikube kubelet[2513]: E0318 15:52:13.200263    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:52:13 minikube kubelet[2513]: E0318 15:52:13.200321    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:52:21.200308988 +0000 UTC m=+848.973278383 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:52:21 minikube kubelet[2513]: E0318 15:52:21.244854    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:52:21 minikube kubelet[2513]: E0318 15:52:21.245098    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:52:37.245063594 +0000 UTC m=+865.018033002 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:52:37 minikube kubelet[2513]: E0318 15:52:37.337235    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:52:37 minikube kubelet[2513]: E0318 15:52:37.337302    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:53:09.337289337 +0000 UTC m=+897.110258732 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:53:09 minikube kubelet[2513]: E0318 15:53:09.402661    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:53:09 minikube kubelet[2513]: E0318 15:53:09.402798    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:54:13.402784387 +0000 UTC m=+961.175753784 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:54:08 minikube kubelet[2513]: E0318 15:54:08.439754    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-66cb9865b5-blpxj" podUID="690702d6-99e8-43c7-9840-18af74ff4160"
Mar 18 15:54:13 minikube kubelet[2513]: E0318 15:54:13.500448    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:54:13 minikube kubelet[2513]: E0318 15:54:13.527421    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert podName:690702d6-99e8-43c7-9840-18af74ff4160 nodeName:}" failed. No retries permitted until 2025-03-18 15:56:15.527400714 +0000 UTC m=+1083.300370109 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert") pod "ingress-nginx-controller-66cb9865b5-blpxj" (UID: "690702d6-99e8-43c7-9840-18af74ff4160") : secret "ingress-nginx-admission" not found
Mar 18 15:54:53 minikube kubelet[2513]: E0318 15:54:53.435813    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context canceled" pod="ingress-nginx/ingress-nginx-controller-66cb9865b5-blpxj" podUID="690702d6-99e8-43c7-9840-18af74ff4160"
Mar 18 15:54:53 minikube kubelet[2513]: I0318 15:54:53.453718    2513 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert\") pod \"ingress-nginx-controller-56d7c84fd4-jfj4m\" (UID: \"e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-jfj4m"
Mar 18 15:54:53 minikube kubelet[2513]: I0318 15:54:53.453862    2513 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-p9mcv\" (UniqueName: \"kubernetes.io/projected/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-kube-api-access-p9mcv\") pod \"ingress-nginx-controller-56d7c84fd4-jfj4m\" (UID: \"e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-jfj4m"
Mar 18 15:54:53 minikube kubelet[2513]: E0318 15:54:53.562322    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:54:53 minikube kubelet[2513]: E0318 15:54:53.567621    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:54:54.067595819 +0000 UTC m=+1001.840565213 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:54:54 minikube kubelet[2513]: I0318 15:54:54.152615    2513 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-qc8cm\" (UniqueName: \"kubernetes.io/projected/690702d6-99e8-43c7-9840-18af74ff4160-kube-api-access-qc8cm\") pod \"690702d6-99e8-43c7-9840-18af74ff4160\" (UID: \"690702d6-99e8-43c7-9840-18af74ff4160\") "
Mar 18 15:54:54 minikube kubelet[2513]: E0318 15:54:54.152745    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:54:54 minikube kubelet[2513]: E0318 15:54:54.152786    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:54:55.152775379 +0000 UTC m=+1002.925744773 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:54:54 minikube kubelet[2513]: I0318 15:54:54.247800    2513 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/690702d6-99e8-43c7-9840-18af74ff4160-kube-api-access-qc8cm" (OuterVolumeSpecName: "kube-api-access-qc8cm") pod "690702d6-99e8-43c7-9840-18af74ff4160" (UID: "690702d6-99e8-43c7-9840-18af74ff4160"). InnerVolumeSpecName "kube-api-access-qc8cm". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 18 15:54:54 minikube kubelet[2513]: I0318 15:54:54.397088    2513 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-qc8cm\" (UniqueName: \"kubernetes.io/projected/690702d6-99e8-43c7-9840-18af74ff4160-kube-api-access-qc8cm\") on node \"minikube\" DevicePath \"\""
Mar 18 15:54:55 minikube kubelet[2513]: E0318 15:54:55.221220    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:54:55 minikube kubelet[2513]: E0318 15:54:55.221746    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:54:57.221722188 +0000 UTC m=+1004.994691594 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:54:55 minikube kubelet[2513]: I0318 15:54:55.783854    2513 reconciler_common.go:299] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/690702d6-99e8-43c7-9840-18af74ff4160-webhook-cert\") on node \"minikube\" DevicePath \"\""
Mar 18 15:54:55 minikube kubelet[2513]: I0318 15:54:55.826019    2513 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="690702d6-99e8-43c7-9840-18af74ff4160" path="/var/lib/kubelet/pods/690702d6-99e8-43c7-9840-18af74ff4160/volumes"
Mar 18 15:54:57 minikube kubelet[2513]: E0318 15:54:57.262379    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:54:57 minikube kubelet[2513]: E0318 15:54:57.262447    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:55:01.262434173 +0000 UTC m=+1009.035403571 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:55:01 minikube kubelet[2513]: E0318 15:55:01.352556    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:55:01 minikube kubelet[2513]: E0318 15:55:01.352619    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:55:09.352605843 +0000 UTC m=+1017.125575238 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:55:09 minikube kubelet[2513]: E0318 15:55:09.449313    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:55:09 minikube kubelet[2513]: E0318 15:55:09.510306    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:55:25.510286463 +0000 UTC m=+1033.283255859 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:55:25 minikube kubelet[2513]: E0318 15:55:25.589871    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:55:25 minikube kubelet[2513]: E0318 15:55:25.596902    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:55:57.589917438 +0000 UTC m=+1065.362886833 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:55:57 minikube kubelet[2513]: E0318 15:55:57.611893    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:55:57 minikube kubelet[2513]: E0318 15:55:57.666736    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:57:01.666681826 +0000 UTC m=+1129.439651229 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:56:56 minikube kubelet[2513]: E0318 15:56:56.475475    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-jfj4m" podUID="e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91"
Mar 18 15:57:01 minikube kubelet[2513]: E0318 15:57:01.756014    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:57:01 minikube kubelet[2513]: E0318 15:57:01.756083    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 15:59:03.756071606 +0000 UTC m=+1251.529041006 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:59:03 minikube kubelet[2513]: E0318 15:59:03.788318    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 15:59:03 minikube kubelet[2513]: E0318 15:59:03.788425    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 16:01:05.78840297 +0000 UTC m=+1373.561372364 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 15:59:12 minikube kubelet[2513]: E0318 15:59:12.769965    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-jfj4m" podUID="e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91"
Mar 18 16:01:05 minikube kubelet[2513]: E0318 16:01:05.878043    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 16:01:05 minikube kubelet[2513]: E0318 16:01:05.878495    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 16:03:07.878472262 +0000 UTC m=+1495.651441663 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 16:01:30 minikube kubelet[2513]: E0318 16:01:30.778643    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-jfj4m" podUID="e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91"
Mar 18 16:03:07 minikube kubelet[2513]: E0318 16:03:07.977209    2513 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 18 16:03:07 minikube kubelet[2513]: E0318 16:03:07.977588    2513 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert podName:e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91 nodeName:}" failed. No retries permitted until 2025-03-18 16:05:09.977569787 +0000 UTC m=+1617.750539184 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-jfj4m" (UID: "e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91") : secret "ingress-nginx-admission" not found
Mar 18 16:03:46 minikube kubelet[2513]: E0318 16:03:46.782277    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-jfj4m" podUID="e29fb25f-ffc9-47f2-aaf2-dfa4ee630a91"


==> storage-provisioner [1dc881559e80] <==
I0318 15:38:56.245470       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0318 15:39:26.749388       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [c1a0da887571] <==
I0318 15:39:35.933608       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0318 15:39:36.285545       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0318 15:39:36.316497       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0318 15:39:36.468483       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0318 15:39:36.468651       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_68875a76-7c0c-4949-8c12-7311bb1fa322!
I0318 15:39:36.518741       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"2294af46-e331-4c58-97a0-871b05234042", APIVersion:"v1", ResourceVersion:"514", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_68875a76-7c0c-4949-8c12-7311bb1fa322 became leader
I0318 15:39:36.852414       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_68875a76-7c0c-4949-8c12-7311bb1fa322!

